Супер! Я оформлю лекцию в Markdown-разметке, подходящей для GitHub: с заголовками, списками, выделением кода, таблицами команд, а также добавлю оглавление.
Скоро будет готово!

# Введение в Docker

## Оглавление
- [Введение](#введение)
- [Что такое Docker?](#что-такое-docker)
- [Установка Docker](#установка-docker)
- [Основные понятия Docker](#основные-понятия-docker)
- [Основные команды Docker](#основные-команды-docker)
- [Создание Docker-образа с Dockerfile](#создание-docker-образа-с-dockerfile)
- [Docker Compose и многоконтейнерные приложения](#docker-compose-и-многоконтейнерные-приложения)
- [Пример: ASP.NET Core + PostgreSQL в Docker](#пример-aspnet-core--postgresql-в-docker)
- [Заключение](#заключение)

## Введение

**Docker** – это платформа для разработки, доставки и запуска приложений в _контейнерах_. Контейнеры позволяют упаковать приложение с его окружением и зависимостями в единый образ, который можно запускать где угодно — на любом сервере или компьютере с поддержкой Docker. Это значительно упрощает развертывание приложений, обеспечивая стабильную работу в разных средах (разработки, тестирования, продакшена) без проблем совместимости.

В этой лекции мы рассмотрим основы Docker: ключевые понятия, основные команды и научимся создавать собственные образы. В качестве практического примера мы создадим контейнеры для веб-приложения на ASP.NET Core и базы данных PostgreSQL, запустим их с помощью Docker Compose и убедимся, как легко можно разворачивать многокомпонентные приложения. Этот материал подойдёт начинающим и разработчикам, желающим познакомиться с Docker на практике.

## Что такое Docker?

Docker – это программное обеспечение для контейнеризации. Контейнеризация похожа на легковесную виртуализацию: каждое приложение запускается изолированно, со всеми необходимыми библиотеками и настройками, **но разделяет ядро операционной системы с хостом**, благодаря чему контейнеры потребляют меньше ресурсов, чем виртуальные машины.

**Ключевые особенности контейнеров Docker:**

- *Легковесность:* контейнеры занимают мало места и запускаются за секунды, так как не требуют полноценной гостевой ОС.
- *Изоляция:* приложение в контейнере не конфликтует с программами на хосте или в других контейнерах (у каждого свой файловый слой, свои переменные окружения, порты и т.д.).
- *Портативность:* образ контейнера можно запускать на любом сервере с Docker – среда исполнения всегда будет идентична. Это решает проблему "работает на моём компьютере".
- *Масштабирование:* с помощью контейнеров легко масштабировать сервисы, запуская несколько экземпляров контейнеров из одного образа.

Docker включает в себя все необходимое для работы с контейнерами: механизм создания **образов**, запуска **контейнеров**, сетевого взаимодействия между ними, управления хранилищами данных (томами), а также готовую экосистему для распространения образов (Docker Hub и другие регистри).

## Установка Docker

Чтобы начать работать с Docker, необходимо установить Docker Engine или Docker Desktop:

- **Windows и macOS:** Загрузите и установите [Docker Desktop](https://www.docker.com/products/docker-desktop) с официального сайта Docker. Docker Desktop включает в себя Docker Engine и дополнительный GUI, позволяющий удобно управлять контейнерами на локальной машине.
- **Linux:** Установите Docker Engine через систему пакетов вашего дистрибутива (например, `apt` в Ubuntu или `yum`/`dnf` в CentOS). Актуальные инструкции для разных дистрибутивов можно найти в официальной документации ([Docker Docs: Get Docker](https://docs.docker.com/get-docker/)).

После установки вы можете проверить, что Docker корректно работает. Выполните в терминале команду:

```bash
docker run hello-world
```

Эта команда загрузит тестовый образ `hello-world` с Docker Hub и запустит контейнер, который выведет приветственное сообщение. Успешный вывод сообщения “Hello from Docker!” означает, что всё установлено правильно.

## Основные понятия Docker

Прежде чем переходить к практическим примерам, познакомимся с базовыми понятиями Docker:

- **Образ (Image):** Шаблон для создания контейнера. Образ содержит всё, что нужно для запуска приложения: файловую систему с установленным приложением, системные библиотеки, настройки окружения и т.д. Образы являются неизменяемыми слоями данных. Вы можете представить образ как некий “снимок” приложения. Например, образ `postgres:15-alpine` содержит PostgreSQL версии 15 на базе Alpine Linux.
- **Контейнер (Container):** Запущенный экземпляр образа. Контейнер – это процесс (или группа процессов) на хост-машине, изолированный через механизмы ядра (namespace, cgroups и пр.). У контейнера есть свой файловый слой (поверх слоёв образа), где могут происходить изменения в ходе работы. Контейнер можно запустить, остановить, удалить – при этом исходный образ остаётся неизменным.
- **Docker Daemon (Docker Engine):** Фоновый процесс (служба) Docker, который управляет образами и контейнерами. Клиент Docker (CLI) отправляет команды демону, который их выполняет: запускает контейнеры, собирает образы и т.д.
- **Docker Hub (и другие реестры образов):** Это облачные хранилища для обмена образами. Docker Hub – самый популярный публичный регистр (registry), где можно найти официальные образы многих приложений (Nginx, Postgres, Redis, Ubuntu и т.д.), а также публиковать собственные образы. Команда `docker pull` позволяет загрузить образ из регистра в локальное хранилище.
- **Dockerfile:** Текстовый файл со скриптом-инструкцией для сборки образа. В Dockerfile перечисляются шаги, как создать образ (например, какая базовая ОС взять, какие файлы скопировать, какие команды выполнить в контейнере при сборке и при запуске). На основе Dockerfile команда `docker build` создаёт новый образ.
- **Том (Volume):** Механизм сохранения данных вне контейнера. Если контейнер удалён, все изменения его файловой системы пропадают, поэтому для сохранения данных (например, базы данных) используют томы. Том – это специальная директория на хосте (или облачное хранилище), которую Docker может примонтировать внутрь контейнера. Таким образом данные переживают пересоздание контейнеров.
- **Сеть (Network):** Docker позволяет создавать виртуальные сети, внутри которых контейнеры могут общаться друг с другом по именам. По умолчанию Docker запускает контейнеры в сети `bridge` (изолированной от внешней сети, с доступом через NAT). Вы также можете создавать свои сети для групп контейнеров (например, объединить контейнер веб-приложения и контейнер базы данных в одну логическую сеть).

## Основные команды Docker

Работа с Docker обычно происходит через командную строку (CLI). Ниже приведён список основных команд Docker и их назначение:

| Команда                              | Описание                                                                             |
|--------------------------------------|--------------------------------------------------------------------------------------|
| `docker pull <образ>`                | Загрузить образ с Docker Hub или другого регистра на локальную машину.               |
| `docker images`                      | Просмотреть список образов, сохранённых локально.                                    |
| `docker run [опции] <образ>`         | Создать и запустить контейнер из указанного образа. Можно добавить опции (например, `-d` для запуска в фоне, `-p <host:container>` для проброса порта, `--name` для имени контейнера). |
| `docker ps`                          | Показать список запущенных контейнеров. (`docker ps -a` покажет все контейнеры, включая остановленные). |
| `docker stop <имя/ID контейнера>`    | Остановить запущенный контейнер (послать SIGTERM, а затем SIGKILL через таймаут).    |
| `docker rm <имя/ID контейнера>`      | Удалить контейнер. (Остановленный контейнер можно удалить этой командой, либо добавить `-f` для принудительного остановка+удаления). |
| `docker build -t <имя_образа> <путь>`| Собрать образ из Dockerfile, находящегося в указанном пути. Ключ `-t` задаёт тег (имя) создаваемого образа. |
| `docker rmi <образ>`                 | Удалить образ (если от него не зависят контейнеры).                                  |
| `docker logs <имя/ID контейнера>`    | Вывести логи (stdout/stderr) контейнера. Полезно для отладки работы приложения внутри контейнера. |
| `docker exec -it <контейнер> <shell>` | Выполнить команду внутри запущенного контейнера. Часто используют `docker exec -it <контейнер> /bin/bash` для открытия интерактивного терминала внутри контейнера Linux. |

> **Note:** Во всех командах вместо `<имя/ID контейнера>` можно использовать либо имя контейнера (если было задано опцией `--name` или через Docker Compose), либо его идентификатор (первые несколько символов ID, который виден в `docker ps`). Для образов аналогично: можно указывать имя с тегом (например, `myapp:latest` или `postgres:15`).

Этих команд достаточно для базовых операций. Далее мы применим некоторые из них на практике, шаг за шагом создавая и запуская контейнеры.

## Создание Docker-образа с Dockerfile

Если вы хотите контейнеризировать своё приложение, необходимо создать **Dockerfile** – файл сценария сборки образа. В Dockerfile описываются все шаги, как собрать ваш образ. Рассмотрим основные инструкции Dockerfile:

- **FROM** – указывает базовый образ, от которого мы отталкиваемся. Обычно это минимальная ОС или платформа (например, Alpine Linux, Ubuntu) или среда выполнения (например, .NET, Node.js, Python и т.д.). Все последующие команды будут выполняться на основе этого базового образа.
- **COPY / ADD** – копирование файлов в образ (например, исходный код вашего приложения, файлы конфигурации и т.д.).
- **RUN** – выполнение команды в процессе сборки образа (например, установка зависимостей, сборка приложения).
- **EXPOSE** – декларация порта, который контейнер будет слушать (это не пробрасывает порт наружу автоматически, но служит документацией; для проброса используется `docker run -p` или настройки в Docker Compose).
- **CMD / ENTRYPOINT** – задают команду, которая будет выполнена при запуске контейнера (например, запуск вашего приложения). `CMD` обычно используется для указания *дефолтных* параметров, а `ENTRYPOINT` – для основного запускаемого процесса. В простых случаях можно использовать что-то одно.

Создадим Dockerfile для примера ASP.NET Core приложения. Предположим, у нас есть веб-приложение на .NET (например, проект ASP.NET Core Web API). Мы хотим собрать образ, который позволит запускать это приложение в контейнере:

```dockerfile
# Используем официальный образ .NET SDK для сборки приложения
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build
WORKDIR /app

# Копируем файлы проекта и восстанавливаем зависимости
COPY *.csproj ./
RUN dotnet restore

# Копируем остальной исходный код и публикуем приложение (Release сборка)
COPY . ./
RUN dotnet publish -c Release -o /app/out

# Используем более лёгкий runtime-образ для финального контейнера
FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS runtime
WORKDIR /app

# Копируем собранное приложение из предыдущего этапа
COPY --from=build /app/out ./

# Указываем порт, который будет слушать наше приложение
EXPOSE 80

# Задаём команду запуска: запускаем .NET приложение
ENTRYPOINT ["dotnet", "MyApp.dll"]
```

Разберём этот Dockerfile:

- Мы используем двухэтапную сборку (**multi-stage build**). Сначала на базе образа SDK (`mcr.microsoft.com/dotnet/sdk`) мы компилируем приложение (в Stage с именем `build`), затем на базе более лёгкого образа ASP.NET Runtime (`mcr.microsoft.com/dotnet/aspnet`) создаём финальный образ с уже скомпилированным приложением. Это позволяет уменьшить размер итогового образа, так как SDK (необходимый для сборки) в нём не остаётся.
- Инструкция `COPY *.csproj ./` копирует файл проекта (csproj) и затем выполняется `dotnet restore` – это заранее подтянет все NuGet-зависимости. Затем копируем остальной код и выполняем `dotnet publish` для сборки приложения в папку `/app/out`.
- На втором этапе (образ runtime) копируем результат публикации (`/app/out`) из первого этапа. 
- `EXPOSE 80` – документируем, что приложение ожидает трафик на порту 80 (обычно ASP.NET Core внутри контейнера будет настроен слушать этот порт).
- `ENTRYPOINT ["dotnet", "MyApp.dll"]` – при старте контейнера будет запущена команда `dotnet MyApp.dll`. (Замените "MyApp.dll" на фактическое имя сборки вашего приложения, если вы повторяете пример на практике.)

Имея Dockerfile, можно собрать образ. Выполните команду сборки (находясь в каталоге с Dockerfile):

```bash
docker build -t myapp:latest .
```

Опция `-t myapp:latest` задаёт имя и тег для образа (в данном случае имя **myapp** и тег **latest**). Точка `.` указывает, что контекст сборки – текущий каталог (Docker использует файлы из текущей папки, включая Dockerfile и всё, что нужно скопировать согласно Dockerfile).

После успешной сборки образ появится в списке `docker images`. Теперь вы можете запустить контейнер из этого образа:

```bash
docker run -d -p 5000:80 --name myapp_container myapp:latest
```

Эта команда запускает контейнер в фоновом режиме (`-d`), пробрасывает порт 80 контейнера на порт 5000 вашего компьютера (`-p 5000:80`), даёт контейнеру имя "myapp_container", и указывает запустить образ `myapp:latest`. 

Если приложение стартовало успешно, контейнер будет в статусе "Up" (можно проверить через `docker ps`). Вы сможете открыть в браузере [http://localhost:5000](http://localhost:5000) и увидеть ваше приложение (например, Swagger UI или какой-либо ответ API), **если** приложение не зависит от внешних сервисов. 

Однако наше примерное приложение ASP.NET Core предполагает подключение к базе данных PostgreSQL. Если сейчас попробовать обратиться к функциональности, требующей базы, то, вероятно, возникнет ошибка подключения – ведь базы данных пока нет. Далее мы рассмотрим, как добавить и подключить контейнер с PostgreSQL.

## Docker Compose и многоконтейнерные приложения

Реальная сила Docker проявляется при создании **композиции** нескольких контейнеров, например, когда ваше веб-приложение работает совместно с базой данных, кешем, очередью сообщений и т.д. Координировать запуск нескольких контейнеров вручную (несколькими командами `docker run` и настройкой сети между ними) не очень удобно. Для этих целей используется инструмент **Docker Compose**.

Docker Compose позволяет описать многоконтейнерное приложение в одном YAML-файле (`docker-compose.yml`), указав, какие образы нужны, какие порты пробросить, какие переменные окружения задать, как тома подключить и т.д. Затем одной командой `docker-compose up` можно запустить сразу все сервисы, определённые в конфигурации.

Преимущества Docker Compose:
- **Удобство конфигурации:** В одном файле задаются все сервисы и их параметры. Конфигурацию можно хранить в репозитории рядом с кодом приложения.
- **Изоляция среды:** Docker Compose по умолчанию создаёт отдельную сеть для сервисов, так что их порты не конфликтуют с другими контейнерами вне этой композиции. Сервисы могут обращаться друг к другу по имени (по названию сервиса в Compose файле).
- **Лёгкий запуск/остановка:** Одна команда запускает всё приложение, и одна команда останавливает. Можно быстро поднять среду разработки или тестирования и так же быстро её свернуть.
- **Масштабирование (scale):** В Docker Compose можно при необходимости запускать несколько экземпляров одного сервиса (например, несколько копий веб-приложения) командой `docker-compose up --scale`.

Мы воспользуемся Docker Compose, чтобы объединить наше ASP.NET Core приложение и PostgreSQL в единый проект и запустить их совместно. 

## Пример: ASP.NET Core + PostgreSQL в Docker

Теперь перейдём к практической части: создадим простой проект, состоящий из веб-приложения на ASP.NET Core (например, Web API) и базы данных PostgreSQL, и запустим их в контейнерах. Предположим, у нас уже есть Dockerfile для приложения (мы написали его в предыдущем разделе). Также убедитесь, что приложение настроено на использование строки подключения из конфигурации (например, в *appsettings.json* определён Connection String с именем "DefaultConnection" или подобным).

Следующие шаги покажут, как настроить и запустить оба контейнера вместе:

1. **Создайте файл конфигурации Docker Compose** (например, `docker-compose.yml`) в корне проекта и опишите в нём два сервиса: `web` (наше приложение) и `db` (PostgreSQL). Вот пример содержимого:

   ```yaml
   version: "3.8"
   services:
     db:
       image: postgres:15-alpine
       environment:
         - POSTGRES_USER=appuser
         - POSTGRES_PASSWORD=secretpassword
         - POSTGRES_DB=myappdb
       volumes:
         - db-data:/var/lib/postgresql/data
       ports:
         - "5432:5432"
     web:
       build: .
       ports:
         - "5000:80"
       environment:
         - ConnectionStrings__DefaultConnection=Host=db;Port=5432;Database=myappdb;Username=appuser;Password=secretpassword
         - ASPNETCORE_URLS=http://+:80
       depends_on:
         - db
   volumes:
     db-data:
   ```

   Разберём этот файл:
   - Определена версия файла Compose (3.8).
   - В секции **services** описаны два сервиса. 
     - **db:** используется готовый образ PostgreSQL (15-й версии, компактная сборка Alpine). Через переменные окружения `POSTGRES_USER`, `POSTGRES_PASSWORD` и `POSTGRES_DB` мы задаём пользователя, пароль и имя базы данных, которые будут автоматически созданы при инициализации контейнера Postgres. Также мы подключаем том `db-data` к стандартной директории хранения данных Postgres внутри контейнера (`/var/lib/postgresql/data`) – это позволит сохранять данные базы вне контейнера. Порт `5432` контейнера проброшен на тот же порт хоста, чтобы при необходимости можно было подключаться к базе извне (например, из вашей ОС для отладки, через pgAdmin или psql).
     - **web:** описывает наше ASP.NET Core приложение. Вместо `image` указана опция **build**, означающая, что образ для этого сервиса нужно собрать из Dockerfile в текущем контексте (`build: .` ищет Dockerfile в текущей директории). Мы пробрасываем порт `80` контейнера на `5000` хоста, чтобы открыть доступ к веб-приложению через http://localhost:5000. Далее, секция `environment` передаёт переменные окружения в контейнер. Мы указываем строку подключения для базы данных: переменная `ConnectionStrings__DefaultConnection` задаёт строку подключения, где `Host=db` — это адрес сервиса базы данных (имя `db` будет резолвиться Docker-Compose'ом внутри сети на контейнер Postgres), порт `5432`, имя базы `myappdb`, пользователь `appuser`, пароль `secretpassword`. **Важно:** внутри контейнера приложения нельзя использовать `Host=localhost` для подключения к Postgres, так как `localhost` в контейнере `web` указывает на него самого. Вместо этого мы используем имя сервиса `db` – Docker Compose автоматически настроит сеть и DNS-имя `db` для контейнера Postgres. Также мы устанавливаем переменную `ASPNETCORE_URLS=http://+:80` – это необходимо, чтобы наше ASP.NET Core приложение внутри контейнера слушало порт 80 (по умолчанию без этой переменной оно могло бы слушать, например, 5000). 
     - `depends_on: - db` означает, что контейнер с приложением `web` не начнёт запускаться, пока не будет запущен контейнер `db`. Это упорядочивает старт, но не гарантирует готовность базы данных к приёму подключений к моменту старта приложения (в простых случаях это не критично, но имейте в виду для продакшн-сценариев).
   - Секция **volumes** в конце определяет named volume `db-data`, который Docker Compose создаст и будет использовать для хранения данных Postgres. При повторных запусках (если volume не удалять) данные в базе сохранятся.

2. **Запустите Docker Compose**. В той же директории, где находится `docker-compose.yml`, выполните команду:

   ```bash
   docker-compose up -d --build
   ```

   Флаг `--build` заставит сначала выполнить сборку образа для сервиса `web` (наш Dockerfile), а затем Docker Compose поднимет оба контейнера. Опция `-d` запускает их в фоновом режиме (detached), чтобы терминал не был занят логами. Если всё прошло успешно, Docker соберёт образ приложения и запустит два контейнера: один для Postgres, другой для ASP.NET Core приложения.

3. **Проверьте состояние контейнеров.** Вы можете выполнить `docker-compose ps` (или классическую `docker ps`) чтобы увидеть список запущенных контейнеров. Оба контейнера `web` и `db` должны быть в состоянии "Up". 

   Также посмотрите логи приложения, чтобы убедиться, что оно подключилось к базе данных без ошибок:
   ```bash
   docker-compose logs web
   ```
   В логах приложения не должно быть ошибок подключения. Для Postgres можно посмотреть логи `docker-compose logs db` – при первом старте он создаст базу и пользователя, указанных в переменных окружения.

4. **Проведите тест приложения.** Теперь приложение должно быть доступно на [http://localhost:5000](http://localhost:5000) (так как мы пробросили порт 5000 -> 80). Откройте браузер и перейдите по этому адресу. Если ваш ASP.NET Core проект представляет собой API, вы можете попробовать вызвать один из его эндпоинтов. В случае, если это шаблон WeatherForecast из стандартного шаблона Web API, перейдите по адресу [http://localhost:5000/WeatherForecast](http://localhost:5000/WeatherForecast). Вы должны получить ответ от сервера (JSON с погодными прогнозами) – значит, контейнерное приложение работает и (если этот запрос требует данные из базы) успешно взаимодействует с базой данных.

5. **Остановка и удаление контейнеров.** Когда вы завершите работу, остановите приложение командой:

   ```bash
   docker-compose down
   ```

   Это остановит контейнеры и уберёт их с сети. Параметр `down` также по умолчанию **не удаляет** volume `db-data` (чтобы данные в PostgreSQL сохранились для следующего запуска). Если вы хотите полностью удалить и данные, можно запустить `docker-compose down -v` (что удалит и подключённые тома).

Наш пример показывает, как с помощью Docker можно упаковать веб-приложение и базу данных в контейнеры и запускать их совместно. Без Docker для запуска такого приложения требовалась бы установка .NET runtime и PostgreSQL на каждой машине, ручная настройка конфигурации — с Docker всё сведено к нескольким шагам: написанию конфигурации и выполнению одной команды для запуска.

## Заключение

Мы познакомились с основами Docker и на практике развернули простое приложение на **ASP.NET Core** с базой **PostgreSQL** в контейнерах. В ходе этого введения были рассмотрены ключевые понятия (образы, контейнеры, Dockerfile, тома, сети), освоены базовые команды Docker для управления контейнерами, а также изучен инструмент **Docker Compose** для многоконтейнерных приложений.

Главные моменты, которые стоит подчеркнуть:
- **Контейнеры Docker** позволяют запускать приложения в изолированной среде, обеспечивая переносимость и повторяемость среды выполнения.
- **Образы** служат шаблонами для создания контейнеров; их можно строить самостоятельно (через Dockerfile) или использовать готовые из Docker Hub.
- **Dockerfile** – мощный способ автоматизировать сборку образа вашего приложения, включая установку всех зависимостей.
- **Docker Compose** упрощает работу с группами контейнеров, позволяя запускать комплекс из нескольких сервисов одной командой и описывать всю конфигурацию в декларативном виде.

Дальнейшие шаги в изучении Docker могут включать ознакомление с продвинутыми темами: оптимизация Dockerfile (многоэтапные сборки, уменьшение размера образов), использование постоянных томов и сетей в различных режимах, оркестрация контейнеров с помощью Kubernetes или Docker Swarm, настройка CI/CD конвейеров для автоматической сборки и доставки образов и т.д. 

Полезные ресурсы для продолжения:
- Официальная документация Docker: <https://docs.docker.com/> (подробные руководства и best practices).
- Docker Hub: <https://hub.docker.com/> (репозиторий образов – ищите официальные образы нужных сервисов).
- Книги и онлайн-курсы по Docker для системных администраторов и разработчиков, где можно углубиться в тему.



<details>
  <summary>Углубленные материалы</summary>

# Docker: Расширенные темы

Данное дополнение к лекции по Docker раскрывает более глубокие аспекты контейнеров и образов, рассчитано на студентов компьютерных наук, уже знакомых с базовыми понятиями Docker. Здесь мы рассмотрим устройство слоёв файловой системы контейнеров, работу сетей Docker, поведение контейнеров при перезапуске, варианты хранения данных, внутреннюю реализацию Docker, работу с логами, а также практические советы по оптимизации.

## Слои образов и контейнеров

 ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/)) *Схема: образ Docker и контейнер в OverlayFS. Слои образа монтируются как read-only `lowerdir`, а слой контейнера – как writable `upperdir`. Объединённое представление файлов доступно в директории `merged`. Если файл присутствует и в образе, и изменён в контейнере, версия из верхнего слоя (`upperdir`) скрывает нижнюю ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=Where%20the%20image%20layer%20and,files%20in%20the%20image%20layer)).* 

Docker-образы состоят из нескольких слоёв, образующих многоуровневую структуру. Каждый слой обычно соответствует инструкции в Dockerfile (например, отдельный слой для каждого `RUN`). При запуске контейнера на основе образа Docker объединяет эти слои с помощью механизма *union-файловой системы* (например, OverlayFS). В результате внутри контейнера мы видим единую файловую систему, собранную из слоёв образа (read-only) и верхнего слоя контейнера (read-write).

**Read-Only и Read-Write слои.** Слои образа монтируются только для чтения. При создании контейнера Docker добавляет новый пустой слой *на запись* поверх слоёв образа ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=To%20create%20a%20container%2C%20the,and%20is%20writable)). Этот верхний слой (слой контейнера) изначально пуст и предназначен для всех изменений внутри контейнера. Таким образом, контейнер может изменять файлы, не затрагивая оригинальные слои образа — все новые и изменённые данные записываются в его собственный слой. Сами слои образа при этом остаются неизменными (immutable) и могут совместно использоваться разными контейнерами. Несколько контейнеров, запущенных из одного образа, будут разделять read-only слои и иметь каждый свой копию верхнего write-layer.

**Механизм Copy-on-Write.** Когда контейнер пытается изменить файл, который находится в нижележащем read-only слое образа, используется стратегия *copy-on-write*. Docker скопирует этот файл в writable-слой контейнера и уже там применит изменения ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=Where%20the%20image%20layer%20and,files%20in%20the%20image%20layer)). В результате контейнер видит изменённый файл, находящийся в его верхнем слое, а оригинальный файл в слое образа остаётся нетронутым (и скрытым верхним файлом). Новые файлы, создаваемые приложением в контейнере, сразу появляются в write-layer. Такой подход обеспечивает эффективность: неизменные данные не дублируются, а разделяются между контейнерами, экономя место. При этом изменённые данные изолированы в контейнере.

**Хранение данных внутри контейнера.** Все изменения файловой системы, сделанные контейнером без использования томов, хранятся в его writable-слое на хосте (например, в каталогах storage-драйвера OverlayFS в `/var/lib/docker/overlay2/`). Эти данные **эфемерны** — если контейнер удалить, его слой на запись тоже удаляется, и вместе с ним пропадают все изменения ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=Since%20the%20files%20created%20or,driver%2C%20which%20reduces%20the%20performance)). Поэтому размер данных, записанных контейнером, не учитывая внешние тома, называется размером слоя контейнера. Проверить его можно командой `docker ps -s` или `docker container ls -s` – она покажет размер writable-слоя (и общий «виртуальный» размер с учётом образа). Если контейнер **перезапустить** (без удаления), его слой на запись сохраняется, и данные не пропадают. Но при полном удалении контейнера (без сохранения тома) все изменения, сделанные внутри него, будут потеряны. Таким образом, по умолчанию файловая система контейнера **не предназначена для долговременного хранения данных** – для этого Docker предлагает механизм томов (volumes), о чём подробнее далее.

## Сети Docker

Docker предоставляет несколько режимов работы сети для контейнеров. Разберём мостовые (bridge) сети, режим host, overlay-сети, а также механизм, с помощью которого контейнеры находят друг друга (внутренний DNS и алиасы).

### Bridge-сеть (мост)

Bridge-сеть – это программный виртуальный коммутатор (мост) внутри хоста, к которому подключаются контейнеры ([Bridge network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/bridge/#:~:text=In%20terms%20of%20networking%2C%20a,within%20a%20host%20machine%27s%20kernel)). По умолчанию Docker создаёт сеть `bridge` (с интерфейсом `docker0`) при запуске демона. Если при запуске контейнера явно не указана сеть, он подключается к этой сети `bridge` по умолчанию. Контейнеры в одной bridge-сети могут свободно общаться друг с другом, но изолированы от контейнеров, не подключённых к этой сети (либо подключённых к другим bridge-сетям). Docker автоматически настраивает правила брандмауэра (iptables) на хосте, запрещая прямое общение между разными сетями.

Особенность bridge-сети – это использование механизма NAT для выхода в наружные сети. Контейнеры получают внутренние IP-адреса (например, в диапазоне `172.17.0.0/16` для default `bridge`) и при попытке обратиться во внешнюю сеть их трафик преобразуется (маскируется) в исходящий от IP хоста ([Going Behind The Scenes of Docker Networking - Argus](https://plaxidityx.com/blog/engineering-blog/docker-networking-behind-the-scenes/#:~:text=But%20what%20exactly%20changed%3F%20What,namespace%20and%20the%20docker0%20bridge)). Благодаря этому контейнеры могут выходить в интернет, даже имея только внутренний адрес. Обратная ситуация – доступ извне к контейнеру – возможна через публикацию порта (порт хоста перенаправляется на порт контейнера). Docker при публикации порта также программирует iptables (правила PREROUTING/POSTROUTING), чтобы трафик на указанный порт хоста перенаправлялся в нужный контейнер.

 ([Going Behind The Scenes of Docker Networking - Argus](https://plaxidityx.com/blog/engineering-blog/docker-networking-behind-the-scenes/)) *Контейнеры Docker, подключённые к виртуальному мосту `docker0`, через пары интерфейсов veth. Мост `docker0` связывает их на уровне 2 (L2) и обеспечивает выход во внешнюю сеть (IPv4/IPv6) через сетевой интерфейс хоста `eth0` посредством NAT ([Going Behind The Scenes of Docker Networking - Argus](https://plaxidityx.com/blog/engineering-blog/docker-networking-behind-the-scenes/#:~:text=But%20what%20exactly%20changed%3F%20What,namespace%20and%20the%20docker0%20bridge)).*

Сеть `bridge` бывает *по умолчанию* (builtin, имя `bridge`) и *пользовательская*. Рекомендуется для своих приложений создавать пользовательские сети типа bridge (`docker network create ...`), вместо использования default-сети. В *пользовательских bridge-сетях* Docker включает встроенный DNS, что позволяет контейнерам резолвить имена друг друга автоматически ([Bridge network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/bridge/#:~:text=%2A%20User,DNS%20resolution%20between%20containers)). Например, если в одной user-defined сети находятся контейнеры с именами `web` и `db`, то внутри контейнера `web` обращение по имени `db` найдет IP адрес контейнера `db` (Docker прозрачно отвечает на DNS-запрос). В **сети по умолчанию** такой возможности нет – контейнеры могут обращаться друг к другу только по IP, либо через устаревший механизм `--link` ([Bridge network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/bridge/#:~:text=Containers%20on%20the%20default%20bridge,other%20by%20name%20or%20alias)). Пользовательские сети также позволяют задавать алиасы для контейнеров и дополнительные параметры (например, отключить внешний доступ, сделав сеть internal). 

Таким образом, bridge-сети удобны для связи контейнеров на **одном хосте**. Для связи между разными хостами они не подходят – контейнеры разных хостов по умолчанию не видят друг друга (нужны либо ручная маршрутизация, либо overlay, см. ниже).

### Сеть host

Режим сети `host` означает, что контейнер **не получает отдельного сетевого стека**, а использует сеть хоста напрямую. Проще говоря, контейнер как бы «садится» на сеть хост-машины, без виртуального интерфейса и без изоляции на уровне L3. С точки зрения сети, запущенный с `--network=host` контейнер эквивалентен обычному процессу на хосте ([Networking using the host network | Docker Docs
](https://docs.docker.com/engine/network/tutorials/host/#:~:text=This%20series%20of%20tutorials%20deals,networking%20topics%2C%20see%20the%20overview)) – например, если он слушает порт 80, то этот порт 80 открыт на самом хосте. Нет дополнительного NAT и оверхеда на маршрутизацию.

**Преимущества host-сети:** отсутствие сетевой прослойки улучшает производительность для сетевых приложений (не тратится время на маршрутизацию через bridge и NAT). Также некоторые приложения или протоколы, работающие с широковещанием или ожидающие конкретного сетевого окружения, могут требовать host-сети. Пример – системы, использующие discovery в локальной сети, или если нужен полный доступ к сетевым интерфейсам хоста.

**Недостатки и риски:** отсутствует изоляция контейнера на уровне сети. Контейнер в host-сети может потенциально прослушивать **весь** трафик хоста, порт может конфликтовать с сервисом на хосте. Если процесс внутри контейнера будет скомпрометирован, он сразу получит доступ к сети хоста. Поэтому host-сеть следует использовать осторожно. Также host-сеть работает только на Linux-хостах (в Docker Desktop для Mac/Windows она доступна с ограничениями, как экспериментальная функция). В большинстве случаев user-defined bridge-сети более безопасны и достаточны.

### Overlay-сеть

Overlay-сети предназначены для объединения контейнеров, запущенных на **разных хостах**, в единую логическую сеть. Драйвер `overlay` создает распределённую сеть поверх (над) физических сетей хостов ([Overlay network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/overlay/#:~:text=The%20,and%20the%20correct%20destination%20container)). Контейнеры, подключенные к одной overlay-сети, могут обмениваться данными, даже если находятся на разных Docker-демонах (машинах). Docker автоматически прокладывает маршруты для пакетов: каждый пакет, отправленный контейнером, доставляется нужному хосту и контейнеру-получателю (с использованием подкапотных туннелей VXLAN или аналогичных технологий).

Особенно актуальны overlay-сети в режиме Docker Swarm (кластере) – они позволяют сервисам (контейнерам) на разных узлах общаться, не требуя ручной настройки маршрутизации. При создании overlay-сети можно указать флаг `--opt encrypted` для шифрования трафика между узлами кластера ([Docker-Swarm - Networking - Learning-Ocean](https://learning-ocean.com/tutorials/docker-swarm/docker-swarm-networking/#:~:text=Docker,encrypted%20flag%20on%20docker)). В Docker Swarm **служебный (control plane) трафик** между менеджерами шифруется по умолчанию, а вот **пользовательский трафик контейнеров** не шифруется, если не включить опцию `encrypted` ([Docker Networking Security Basics | dockerlabs - Collabnix](https://dockerlabs.collabnix.com/advanced/security/networking/#:~:text=Collabnix%20dockerlabs,encrypted%20flag%20to%20the)). С включенным шифрованием данные между контейнерами шифруются (ESP/IPsec) поверх UDP-туннеля. Это полезно при передаче чувствительных данных между датацентрами или через небезопасные сети.

Для использования overlay-сетей вне Swarm (между отдельными Docker-демонами) требуется запустить Docker в режиме Swarm даже для stand-alone контейнеров, и создать overlay-сеть с опцией `--attachable`. Пример: `docker network create -d overlay --attachable my-net`. После этого можно запускать контейнеры с `--network my-net` на разных хостах (включённых в один Swarm) – они окажутся в общей сети. Docker сам откроет необходимые порты на хостах (2377, 7946, 4789 и др.) и будет управлять overlay-туннелями.

Overlay-сети упрощают архитектуру: разработчику не нужно настраивать маршрутизацию или проброс портов между узлами – Docker берёт это на себя ([Overlay network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/overlay/#:~:text=The%20,and%20the%20correct%20destination%20container)). Все контейнеры в одной overlay-сети обычно находятся в едином IP-пространстве и могут резолвиться по именам (так же как в bridge-сети, есть DNS). Масштабирование сервисов в Swarm также использует overlay: Docker может на уровне DNS распределять запросы между репликами сервисов (Virtual IP + встроенный load balancing).

**Минусы overlay:** небольшое падение производительности из-за накладных расходов (капсулирование VXLAN, шифрование). Но для большинства сценариев это приемлемо. Также стоит помнить об ограничениях: например, по умолчанию не более ~1000 эндпоинтов на хост в одной overlay-сети (лимитации ядра Linux) ([Overlay network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/overlay/#:~:text=Due%20to%20limitations%20set%20by,located%20on%20the%20same%20host)).

### Поиск и коммуникация контейнеров (DNS и алиасы)

Когда контейнеры находятся в одной пользовательской сети (bridge или overlay), Docker предоставляет внутренний DNS-сервер, позволяющий контейнерам находить друг друга по имени. Имя контейнера автоматически регистрируется в DNS этой сети. Например, контейнер `app` может обратиться к контейнеру `db` по hostname `db` – резолв вернёт внутренний IP контейнера `db` ([Overlay network driver | Docker Docs
](https://docs.docker.com/engine/network/drivers/overlay/#:~:text=Publishing%20ports%20of%20a%20container,lookup%20using%20the%20container%20name)). Дополнительно, можно задавать *алиасы* (дополнительные имена) при подключении к сети, например, чтобы один контейнер был доступен под общим именем сервиса.

В Compose-файлах и Swarm-сервисах это ещё упрощается: имя сервиса становится DNS-именем. Например, сервис `database` будет доступен контейнерам фронтенда по адресу `database` (либо `<service_name>` или `<service_name>.<network_name>`). Docker DNS также умеет round-robin между несколькими контейнерами сервиса (в Swarm по умолчанию выдаётся Virtual IP и работает балансировка).

В дефолтной сети `bridge` автоматического DNS-резолвинга **нет**, поэтому без user-defined сети контейнеры необходимо либо линкировать (`--link`, что добавляет записи в `/etc/hosts`), либо вручную прописывать IP. Такой подход считается устаревшим и менее гибким.

Важно отметить, что DNS-имена видны **только внутри соответствующей сети**. Если контейнеры в разных сетях, они не узнают о именах друг друга. Можно подключить один контейнер к нескольким сетям – тогда у него может быть несколько имен (по одному на каждую сеть).

**Итог:** Docker облегчает связи между контейнерами через имена, избавляя от необходимости хардкодить IP-адреса. Для этого рекомендуется использовать пользовательские сети (bridge/overlay). Имя контейнера (или алиас) служит как hostname, резолвящийся через Docker DNS. Если нужно более сложное именование или объединение разных сетей, можно настроить внешний DNS или использовать сервис-дискавери, но в большинстве случаев встроенного механизма достаточно.

## Поведение контейнеров при перезапуске и сбоях

В реальных сценариях контейнеры могут неожиданно завершиться или хост перезагрузиться. Docker позволяет задавать политики рестарта контейнеров и автоматически восстанавливать их. Также важно понимать, что происходит с подключёнными томами и сетями при перезапуске или пересоздании контейнера, и как сохраняются (или меняются) сетевые настройки, IP и имена.

### Политики рестарта контейнеров

При запуске контейнера командой `docker run` можно указать флаг `--restart` с одной из политик автоперезапуска. Основные варианты рестарт-политик ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=Flag%20Description%20,listed%20in%20restart%20policy%20details)):

- **`no`** – не перезапускать контейнер автоматически. Это поведение по умолчанию: Docker не предпринимает попыток перезапуска, если контейнер остановился или упал.
- **`on-failure`** (возможен формат `on-failure[:число]`) – перезапускать контейнер при аварийном завершении, т.е. если процесс внутри контейнера вернул ненулевой код выхода (ошибка). Например, `--restart on-failure:5` перезапустит контейнер не более 5 раз подряд при падениях. Если контейнер остановлен вручную (`docker stop`), эта политика не перезапускает его. Также при перезапуске самого Docker-демона контейнер с on-failure **не** стартует заново, если до этого был остановлен без ошибки ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=,container%20if%20the%20daemon%20restarts)).
- **`always`** – всегда перезапускать контейнер при остановке, независимо от причины (ноль или ненулевой код выхода). Такой контейнер будет автоматически запущен заново даже если его процесс завершился успешно. Исключение – если контейнер был остановлен вручную, Docker не будет сразу его перезапускать, но если демон Docker перезапустится, контейнер опять поднимется ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=with%20a%20failure,even%20after%20Docker%20daemon%20restarts)). То есть `always` гарантирует, что контейнер запущен при работе демона Docker.
- **`unless-stopped`** – почти то же, что `always`, **кроме** случая, когда контейнер был явно остановлен. В этом случае Docker не будет запускать его после перезагрузки демона, пока вы его сами не стартанёте ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=it%27s%20restarted%20only%20when%20Docker,even%20after%20Docker%20daemon%20restarts)). Проще: “перезапускай, пока контейнер не остановили намеренно”. Эта политика удобна тем, что случайный рестарт хоста/демона восстановит контейнер, но если админ его остановил, Docker не будет навязывать запуск.

Пример запуска контейнера с политикой рестарта:

```bash
# Запуск nginx, который будет автоматически перезапускаться, пока его явно не остановят
docker run -d --restart unless-stopped --name webserver nginx:latest
```

Политики рестарта позволяют реализовать простейший self-healing для контейнеров: например, если приложение упало из-за разовой ошибки, Docker его перезапустит. Однако, важно следить, чтобы контейнер вообще способен нормально стартовать; Docker ждёт ~10 секунд после запуска, прежде чем начать применять политику (если контейнер падает мгновенно, бесконечный цикл рестартов прерывается ограничением) ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=,going%20into%20a%20restart%20loop)). Также, если контейнер постоянно крашится, политика `always` может привести к циклу перезапусков – лучше в таких случаях использовать `on-failure` с ограничением попыток.

Отметим, что политики рестарта применяются только к контейнерам (для Swarm-сервисов в docker-compose используются свои параметры рестарта, отличные от этих). Если контейнер запущен в режиме foreground (не `-d`), то при его падении управление вернётся в терминал, несмотря на политику (т.е. в интерактивном режиме auto-restart не удерживает ваш CLI открытым) ([Start containers automatically | Docker Docs
](https://docs.docker.com/engine/containers/start-containers-automatically/#:~:text=Restarting%20foreground%20containers)).

### Томы и сети при пересоздании контейнера

**Тома (volumes):** Один из ключевых плюсов томов в Docker – их независимость от жизненного цикла контейнера. Если контейнер использовал *именованный том* или bind mount, эти данные сохранятся на хосте даже после удаления контейнера. Например, мы запустили контейнер с `-v mydata:/data`. Если удалить контейнер командой `docker rm`, том `mydata` никуда не денется – Docker не удаляет тома автоматически (если не указать `-v` в `docker rm`, что специально удалит анонимные томы). При запуске нового контейнера с тем же томом данные будут на месте. Это справедливо и для bind mount: данные лежат в указанной директории хоста и никуда не денутся, пока вы сами их не удалите.

По-другому обстоит дело с данными, которые были только внутри контейнера (в его write-layer) и **не вынесены в том**. Эти изменения “привязаны” к контейнеру. Если контейнер удалить, то и этот слой удаляется – данные пропадают ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=Since%20the%20files%20created%20or,driver%2C%20which%20reduces%20the%20performance)). Поэтому для долговременных данных (например, база данных, файлы пользователей) нужно использовать тома. Пересоздание контейнера из образа с подключением старого тома позволяет сохранить состояние.

**Сети:** Пользовательские сети Docker (bridge/overlay) существуют независимо от контейнеров. Когда контейнер подключается к сети, Docker создает для него сетевой интерфейс (veth) и регистрирует в сеть. При удалении контейнера Docker убирает его endpoint из сети, но сама сеть (если она user-defined) продолжает существовать. Если создать новый контейнер и подключить к той же сети (например, через `docker run --network my-net`), то для него будет создан новый endpoint. Таким образом, удаление всех контейнеров из сети не удаляет сеть – её надо удалить явной командой `docker network rm`, либо `docker system prune` удалит неиспользуемые сети.

При **перезапуске** контейнера (через `docker restart` или автоматический рестарт), контейнер даже не покидает сеть: Docker сохраняет его сетевые настройки. Контейнер получит тот же IP-адрес и hostname, поскольку это всё тот же инстанс (ID) контейнера. Внутри контейнера после рестарта сохраняется прежнее окружение (файловая система, тома подключены, сети подключены).

А вот при **пересоздании** (удалить и заново run) поведение другое: новый контейнер получит новый IP внутри сети. Даже если задать ему то же имя, IP может отличаться. Обычно нам всё равно, так как мы обращаемся по имени (DNS обновится на новый IP). Но закладываться на статичность IP контейнеров не стоит – Docker не гарантирует выдачу того же адреса.

Hostname по умолчанию у контейнера – это сокращённый ID контейнера. При новом запуске ID меняется, соответственно hostname меняется. Если нужно задать hostname вручную (через `--hostname`), это значение сохранится внутри контейнера, но с точки зрения сети Docker лучше использовать имена контейнеров/алиасы, а не статические hostname.

**Volumes и пересоздание:** часто бывает, что мы хотим обновить контейнер на новую версию образа, сохранив данные. Правильный путь – запуск нового контейнера с подключением тех же томов. Поскольку том живёт на хосте, новые контейнеры могут взять существующий именованный том по имени. Например, ранее для MySQL использовался том `mysql_data`. Обновляем образ: `docker run -d --name newmysql -v mysql_data:/var/lib/mysql mysql:newversion` – база данных внутри нового контейнера подхватит данные из тома.

**Сеть и пересоздание:** аналогично, если новый контейнер запустить в той же пользовательской сети, другие контейнеры смогут его найти по имени. Однако, стоит убедиться, что старый экземпляр или его имя не конфликтуют. Docker не позволит запустить два контейнера с одним именем одновременно – старый надо или остановить и удалить, или запустить новый с другим именем (либо использовать docker-compose, где имя сервиса остаётся прежним, а контейнеры меняются прозрачно).

### IP и hostname при перезапуске

Как отмечалось, при *мягком перезапуске* (`docker restart` того же контейнера) его IP-адрес в сети, MAC-адрес и прочие сетевые идентификаторы сохранятся, потому что контейнер никуда не «уходит» из сети. Это удобно: временной перезапуск не влияет на связи между контейнерами (они по-прежнему найдут его по имени, IP тот же).

Если же контейнер **упал и Docker его перезапустил** (по политике `always` или `on-failure`), то ситуация аналогична: контейнер не был удалён, Docker просто пересоздал процесс внутри, поэтому сетевые настройки сохраняются.

Но при **удалении** контейнера и запуске нового: 
- Меняется имя (если вы задали то же имя, то старый должен быть удалён, иначе конфликт). DNS в сети обновится, но на короткое время может быть ситуация, что имя не резолвится, пока новый контейнер не подключён.
- Меняется IP. Если кто-то обращался по IP, он станет недействителен. Поэтому лучше так не делать, всегда обращаться по имени.
- Hostname (внутри контейнера) – если не указан явно, будет новый (ID нового контейнера). Если указан явно через `--hostname`, у нового контейнера надо заново указать, Docker не «наследует» hostname старого контейнера по имени.

Docker не обеспечивает сохранение **состояния соединений** при рестартах. Т.е. если контейнер перезапустился, все TCP-сессии к нему обрываются и требуются переподключения.

Для долгоживущих сервисов, работающих в кластере, обычно поверх Docker делают уровень оркестрации (Swarm, Kubernetes), где при пересоздании контейнеров перенаправляются потоки, и сервис продолжает работу. Но на уровне Docker-демона – container заменяется новым экземпляром, о чём клиенты должны знать (например, в Compose можно делать rolling update, но это уже выходит за рамки данного материала).

**Вывод:** static IP контейнера не гарантируется, static hostname – только если явно задать, но обычно в этом нет необходимости. Используйте названия сервисов/контейнеров для связи, а для устойчивости при рестарте — политики `restart` (чтобы Docker сам поднимал контейнеры после сбоев или перезагрузки).

## Разновидности хранилищ в Docker

Docker предоставляет несколько способов хранения данных: *именованные тома* (volumes), *прямое монтирование директорий хоста* (bind mounts), а также *tmpfs*-тома (в памяти). Понимание различий между ними важно для правильного выбора подхода к хранению. Также рассмотрим, где Docker хранит данные томов на хосте.

### Именованные volume vs bind mount

| Характеристика          | Именованный том (Volume)                     | Bind mount (примонтированный каталог)          |
|-------------------------|----------------------------------------------|-----------------------------------------------|
| Создание/подключение    | Управляется Docker по имени тома (Docker сам создаёт папку) | Использует указанный путь на хосте (должен существовать или будет создан) |
| Расположение данных     | `/var/lib/docker/volumes/<имя>/_data` на хосте ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=)) | Любой путь на файловой системе хоста (как указано) |
| Управление              | Docker CLI (`docker volume ls/inspect/rm`)   | Вне Docker (работа с обычной папкой хоста)     |
| Портативность           | Независимы от конкретного пути хоста (имя можно перенести между машинами вместе с содержимым) | Зависит от окружения хоста (нужен тот же путь и данные) |
| Совместимость           | Работают на Linux и Windows контейнерах (Docker сам управляет) | Зависят от ОС хоста (путь должен существовать, права доступа и т.д.) |
| Доступ с хоста          | Не предназначен для прямого доступа (хотя возможен, но не рекомендуется менять файлы вручную) ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=Volumes%20are%20also%20stored%20as,data%20in%20Docker%20is%20Volumes)) | Полный доступ к файлам напрямую на хосте (можно редактировать вне контейнера) |
| Производительность      | Высокая – запись минует слои Copy-on-Write (прямая запись на хостовой ФС) ([Volumes | Docker Docs
](https://docs.docker.com/engine/storage/volumes/#:~:text=Volumes%20are%20often%20a%20better,directly%20to%20the%20host%20filesystem)) | Высокая – запись сразу на хостовую ФС (как с обычным каталогом) |
| Типичные сценарии       | Долговременные данные контейнеров (БД, загрузки, конфиги, требующие сохранности между перезапусками) | Интеграция с файловой системой хоста (например, использовать исходники проекта внутри контейнера, доступ к конфигам хоста) |

**Volume (именованный том):** создаётся либо командой `docker volume create`, либо автоматически при запуске контейнера с флагом `-v имя:путь` (если том с таким именем не существует, Docker создаст). Docker сам выбирает, где хранить данные – обычно локально в `/var/lib/docker/volumes/`. Volume изолирован от остальной системы (не монтируется никуда, кроме как в контейнер по запросу). Это удобный способ сохранить данные между запусками контейнера, не завися от конкретных путей. Можно легко передать том другому контейнеру, использующему тот же путь внутри. Также volume можно подключить к нескольким контейнерам (чтобы они разделяли данные) – например, несколько контейнеров могут читать/писать в один том (Docker не запрещает, но нужно самим учитывать конкурирующий доступ). 

Volume не увеличивает размер самого контейнера: если приложение пишет на volume, эти данные не попадают в writable-layer контейнера ([Volumes | Docker Docs
](https://docs.docker.com/engine/storage/volumes/#:~:text=Volumes%20are%20often%20a%20better,directly%20to%20the%20host%20filesystem)), что ускоряет операции (без overlay-надстроек). Поэтому volumes – предпочтительный способ работы с большими объёмами данных.

**Bind mount:** предоставление контейнеру доступа к конкретной директории хоста. Указывается флагом `-v /host/path:/container/path` или с новым синтаксисом `--mount type=bind,source=/host/path,target=/container/path`. Этот способ даёт полный контроль над тем, куда именно на хосте пишутся данные контейнера. Используется часто при разработке (монтировать исходники внутрь контейнера, чтобы приложение в контейнере видело изменения), или для доступа к файлам, которые должны сохраняться в определённом месте на хосте (например, логи в /var/log/host, загрузки и пр.). Bind mount напрямую отражает состояние хоста: права доступа, наличие или отсутствие файлов – всё влияет на контейнер.

**Безопасность:** bind mount потенциально опаснее – контейнер (особенно с правами root) получает доступ к файловой системе хоста в указанной точке. Если смонтировать системные директории, контейнер может повредить хост. Поэтому использовать bind к чувствительным путям надо осторожно. Именованные volume изолированы – процессы вне Docker не трогают их (если специально не заморачиваться).

**Когда что использовать:** 
- Если вам не важно, где на хосте хранятся данные, и вы хотите, чтобы Docker сам всё организовал – берите volume. Это проще для бэкапов (можно `docker volume inspect` увидеть путь, или вообще `docker cp` работать с томом через временный контейнер). Volumes – основной инструмент в продакшене для баз данных, т.к. они легко мигрируют и не зависят от среды.
- Если нужен чёткий контроль и прямой доступ – например, вы хотите открыть конфигурационный файл хоста внутри контейнера – тогда bind mount. Также при разработке: монтирование кода. Ещё пример – нужно загрузить/выгрузить файлы в контейнер в ходе работы, и проще положить их на хост (куда-то в общую папку) и сделать bind.
- В Windows-средах: volumes работают и там (хранятся либо внутри WSL2 VM, либо по-иному), а bind mounts позволяют монтировать, например, `C:\path` внутрь Linux-контейнера (с ограничениями). В Linux в целом нет больших проблем с обоими способами.

Наглядный пример сравнения:

```bash
# Именованный том: сохраняем данные MySQL на томе, который Docker сам хранит
docker volume create mysql_data
docker run -d --name db1 -v mysql_data:/var/lib/mysql mysql:latest

# Удаляем контейнер, потом запускаем новый с тем же volume:
docker rm -f db1
docker run -d --name db2 -v mysql_data:/var/lib/mysql mysql:latest
# -> новый контейнер db2 использует те же данные БД из тома mysql_data
```

```bash
# Bind mount: сохраняем данные MySQL в конкретной папке хоста
mkdir -p /home/user/mysql_data   # создание директории для данных
docker run -d --name db3 -v /home/user/mysql_data:/var/lib/mysql mysql:latest

# Проверяем на хосте
ls /home/user/mysql_data   # увидим файлы базы данных MySQL (ibdata, ib_logfile и др.)

# Удаляем и пересоздаём контейнер, данные на месте:
docker rm -f db3
docker run -d --name db4 -v /home/user/mysql_data:/var/lib/mysql mysql:latest
```

Оба способа выше обеспечат сохранение данных при перезапусках/удалении контейнера. Разница в том, где хранятся данные и как к ним можно получить доступ извне.

### Tmpfs-том (tmpfs volume)

Tmpfs-том – это особый тип volume, который хранится в оперативной памяти и никогда не записывается на диск. По сути, это раздел в RAM, смонтированный в контейнер (через tmpfs Linux). Удобно для временных данных, которые должны быть очень быстры в доступе или не должны остаться на диске (например, кэш, временные вычисления, секретные данные, которые не нужны после перезагрузки). 

Использовать: либо флаг `--tmpfs /container/path` у `docker run` (с возможными опциями размера, флагов mount), либо в Docker Compose:

```yaml
services:
  app:
    image: myapp
    tmpfs:
      - /tmp:size=100m
```

Это смонтирует внутри контейнера пустую директорию `/tmp`, хранящую данные в памяти (до 100 МБ). При остановке/удалении контейнера всё содержимое tmpfs теряется. Производительность tmpfs обычно выше, чем у SSD/HDD, но увлекаться не стоит: слишком большой tmpfs может отъесть значительную часть RAM у хоста.

Use-case: например, веб-сервер мог бы использовать tmpfs для кэша сессий, если потеря кэша при рестарте некритична, зато высокая скорость доступа. Или обработка файлов: считать гигантский JSON, обработать в памяти, не записывая на диск – можно монтировать tmpfs для промежуточных данных.

Ограничение: tmpfs доступен только на Linux-контейнерах (в Docker Desktop для Mac/Win – контейнеры всё равно Linux внутри, так что работает). 

### Где Docker хранит тома на хосте

По умолчанию Docker (в Linux) сохраняет данные именованных томов в директории: `/var/lib/docker/volumes/`. На каждый volume приходится подпапка с именем тома, внутри которой находится `_data` – там и лежат файлы тома ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=)). Пример вывода `docker volume inspect` для именованного тома:

```json
"Name": "mysql_data",
"Driver": "local",
"Mountpoint": "/var/lib/docker/volumes/mysql_data/_data",
"Scope": "local"
```

Как видно, `Mountpoint` указывает, где на хосте находятся данные. Если открыть этот путь, увидим файлы, что и внутри контейнера на томе.

Docker рекомендует **не менять вручную** содержимое `/var/lib/docker/volumes/...` ([Understanding Docker Volumes: Storage Locations Demystified](https://www.ecloudcontrol.com/docker-volumes-bind-mounts-tmpfs/#:~:text=Volumes%20are%20also%20stored%20as,data%20in%20Docker%20is%20Volumes)). Именованные тома предполагается модифицировать через контейнеры (или команды Docker). В экстренных случаях, конечно, никто не запрещает прочитать данные непосредственно, но аккуратнее с правами – обычно всё там принадлежит root.

Для **bind mount** нет специального места – вы сами определяете путь, и Docker просто монтирует его. Поэтому ответственность за существование и резервное копирование этих данных на вас.

В Windows и Mac (Docker Desktop): под капотом данные тоже хранятся, как правило, внутри виртуальной машины. Например, в Docker Desktop есть раздел, соответствующий `/var/lib/docker/volumes/` внутри VM. Однако, Docker Desktop также позволяет монтировать Windows-папки как bind mount.

**Удаление томов:** Когда volume больше не нужен (ни один контейнер его не использует), он будет “висеть” на диске, пока его не удалить. Можно очистить все неиспользуемые тома командой `docker volume prune` (об этом далее). Named volume, явно созданный и больше не нужный, можно удалить `docker volume rm имя`. (Если том все еще примонтирован к контейнеру, Docker не даст удалить, надо сперва контейнер убрать или отмонтировать).

**Расположение образов и контейнеров:** (хотя это больше к внутреннему устройству, но упомянем) – Docker хранит все слои образов и контейнеры (их данные) тоже под `/var/lib/docker/`. Например, при драйвере overlay2 – в `/var/lib/docker/overlay2/` находятся директории, соответствующие каждому слою образа и каждому контейнеру.

## Внутреннее устройство Docker

В этом разделе рассмотрим технические детали: как Docker хранит образ и слои контейнера на диске (на примере драйвера storage **overlay2**), какие каталоги используются Docker’ом на хосте, и как очищать ненужные данные (prune).

### Драйвер overlay2 и Copy-on-Write на уровне ОС

Современные установки Docker на Linux по умолчанию используют storage-driver **overlay2** (требует поддержу OverlayFS в ядре). OverlayFS – файловая система, позволяющая накладывать одну директорию на другую (overlay), реализуя как раз механизм UnionFS для Docker. 

Когда мы загружаем (pull) Docker-образ, каждый его слой сохраняется как каталог с файлами этого слоя в `/var/lib/docker/overlay2/` ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=edab9b5e5bf73f2997524eebeac1de4cf9c8b904fa8ad3ec43b3504196aa3801)). В этих каталогах хранятся:
- `diff/` – содержимое файловой системы слоя (новые или изменённые файлы относительно предыдущего слоя).
- `lower` (у неглавных слоёв) – ссылка на ID родительского слоя.
- `merged` (у контейнеров или смонтированных образов) – точка монтирования, где слои объединяются.
- `work` – служебная папка для работы OverlayFS.

Когда Docker запускает контейнер из образа, он делает следующее: берет последний слой образа (верхний read-only слой) и монтирует на него пустую папку для записи (upperdir) – это и будет слой контейнера ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=To%20create%20a%20container%2C%20the,and%20is%20writable)). Также монтируются все нижележащие слои образа (каждый как lowerdir) – OverlayFS умеет принимать несколько lowerdir. Результат монтирования – объединённая файловая система (mount point), которую контейнер видит как `/` внутри себя. В этой объединённой ФС при совпадении файлов приоритет у верхнего слоя (контейнера) ([OverlayFS storage driver | Docker Docs
](https://docs.docker.com/engine/storage/drivers/overlayfs-driver/#:~:text=Where%20the%20image%20layer%20and,files%20in%20the%20image%20layer)).

Например, если образ имеет файл `/etc/config` и контейнер изменяет его, то произойдёт:
- В каталоге upperdir контейнера появляется копия `etc/config` (уже изменённая).
- При чтении `/etc/config` внутри контейнера OverlayFS сначала смотрит upperdir, находит там файл и отдаёт его, игнорируя нижний.

OverlayFS на диск записывает только изменения (копия + дельта). Если контейнер создал новый файл, он просто лежит в upperdir. Если ничего не менял, upperdir может быть почти пустым.

Сами каталоги overlay2 на хосте выглядят как набор длинных хэшированных идентификаторов. Связи между ними Docker хранит в метаданных (например, соотнесение ID слоя образа с его parentами). Не стоит напрямую пытаться что-то редактировать там – можно нарушить целостность образов.

Помимо overlay2, Docker поддерживает и другие storage drivers: aufs (раньше часто на Ubuntu), devicemapper (в некоторых системах, основанных на LVM, раньше применялся), btrfs, zfs и др. Принципиально они все обеспечивают похожую модель многослойных образов и CoW, но детали реализации разные (например, devicemapper использует thin pool LVM, каждый слой – том в thin pool). Overlay2 признан одним из наиболее быстрых и эффективных, поэтому вытеснил aufs.

### Структура данных Docker на хосте (`/var/lib/docker/`)

Рассмотрим важные директории, которые Docker демону нужны для хранения состояния (по умолчанию база данных Docker находится именно в `/var/lib/docker`, но путь может быть изменён опцией `--data-root`).

- **`/var/lib/docker/overlay2/`** – как упомянуто, здесь хранится содержимое слоёв образов и контейнеров (при драйвере overlay2). Много папок с хэшами; Docker `inspect` может показать, какие именно ID соответствуют слою образа или контейнера.
- **`/var/lib/docker/containers/`** – папки с метаданными и данными контейнеров. Название папки = длинный ID контейнера. Внутри:  
  - файл `config.v2.json` – настройки контейнера (точная конфигурация, которую Docker использует для запуска: команды, точки монтирования, сеть и т.д.),
  - `hostconfig.json` – параметры, специфичные для хоста (например, лимиты ресурсов, привязки устройств),
  - несколько симлинков на слои, 
  - и **логи** по умолчанию: файл `<ID>-json.log` (если используется драйвер json-file). Именно оттуда `docker logs` читает вывод ([Docker Logging: 101 Guide to Logs, Best Practices & More](https://sematext.com/guides/docker-logs/#:~:text=log%20file%20contains%20information%20about,only%20one%20container)).
- **`/var/lib/docker/volumes/`** – данные именованных томов, как обсуждалось выше. Каждый том – папка.
- **`/var/lib/docker/image/`** – метаданные образов. Внутри по имени драйвера (`overlay2/`, `aufs/` и т.п.) – информация о слоях, их родительских связях, тегах образов. Тут нет больших данных, только JSONы и служебные файлы.
- **`/var/lib/docker/network/files/`** – конфигурации сетей (например, файл локальной конфигурации bridge-сети).
- **`/var/lib/docker/swarm/`** – (если используется Swarm) хранит состояние Swarm-кластера (CA, ключи, состояния DB raft).

Docker также может использовать `/etc/docker/` для конфигурации демона (daemon.json), а контейнеры могут иметь данные в `/run/docker/...` (runtime-сокеты и пр.), но основное – это `/var/lib/docker`.

Знание этой структуры помогает, например, при отладке: почему место на диске занято – можно зайти и увидеть крупные файлы (часто гигабайты именно в overlay2 или volumes). Но **не рекомендуется** вручную чистить эти папки – лучше использовать утилиты Docker для этого.

### Очистка неиспользуемых данных (prune)

Со временем Docker-хост может накопить много “мусора”: остановленные контейнеры, неиспользуемые тома, образы старых версий, «dangling» образы (слои без тега), кэш сборок. Для уборки Docker предоставляет команду `docker system prune` и более точечные prune-команды.

- **`docker system prune`** – комплексная очистка всего, что не используется:
  - удаляет остановленные контейнеры,
  - удаляет неиспользуемые сети (пользовательские сети без контейнеров),
  - удаляет **висячие образы** (*dangling images* – слои образов, не имеющие тега, обычно остатки неудачных сборок или промежуточные слои) ([Prune unused Docker objects - Docker Docs](https://docs.docker.com/engine/manage-resources/pruning/#:~:text=Prune%20unused%20Docker%20objects%20,A%20dangling)),
  - (начиная с Docker 17.06) удаляет и кастомные образы, не привязанные ни к одному контейнеру, если указать флаг `-a` (`--all`),
  - **не удаляет тома** по умолчанию, но можно добавить флаг `--volumes`, чтобы включить и неиспользуемые тома.
  
  При запуске без флагов будет запрошено подтверждение. Команда сообщает, сколько всего освободила.
  
- **`docker image prune`** – очистка образов. По умолчанию удаляет только dangling images (слои, не имеющие имени/тега) ([Prune unused Docker objects - Docker Docs](https://docs.docker.com/engine/manage-resources/pruning/#:~:text=Prune%20unused%20Docker%20objects%20,A%20dangling)). Если добавить `-a`, удалит все образы, которые не используются ни одним контейнером (то есть если у вас есть имидж, от которого нет запущенных или даже остановленных контейнеров, он будет удалён). Можно фильтровать по тегам, меткам и пр. Обычно `docker image prune -a` освобождает место от старых версий, оставив только образы тех контейнеров, что ещё существуют.
- **`docker container prune`** – удаляет все остановленные контейнеры. Останавливать принудительно ничего не будет – только уже *Exited*. Это частный случай system prune.
- **`docker volume prune`** – удаляет все не примонтированные ни к одному контейнеру volumes. Очень полезно после экспериментов: можно насоздавать анонимных томов (флаг `-v /path` без имени создает volume с random ID, который потом никуда не делся бы).
- **`docker network prune`** – удаляет неиспользуемые сети (исключая специальную `bridge`, `host`, `none`).
- **`docker builder prune`** – сносит кэш сборки (слои, хранящиеся как кэш Dockerfile). Это иногда тоже значительное место.

**Совет:** перед агрессивной очисткой можно посмотреть сколько вообще пространства занято образами, контейнерами, томами с помощью `docker system df`. Эта команда выведет сводку по использованию диска, сколько образов и какого размера, сколько из них можно очистить, и т.д. Например, она покажет количество «reclaimable» (восстанавливаемого) места.

При использовании prune в продакшене осторожно: убедитесь, что удаляемые образы действительно не нужны (например, `docker image prune -a` удалит все образы, кроме тех, что используются _запущенными_ контейнерами – если у вас есть остановленный контейнер, и вы хотели бы его снова запустить, образ может пропасть). Обычно в продакшене образы хранятся в регистри, и потеря локального экземпляра не критична, но имейте это в виду.

Регулярная очистка полезна в dev/test средах, где быстро накапливаются старые версии. Также CI/CD часто включает `docker system prune` чтобы не разрастался runner.

**Пример:** очистить всё, включая тома:
```bash
docker system prune --volumes -f
```
Опция `-f` (force) убирает запрос подтверждения.

## Работа с логами и подключение внутрь контейнера

При отладке и мониторинге контейнеров часто нужно: посмотреть логи приложения и, при необходимости, зайти «внутрь» контейнера. Рассмотрим команды `docker logs`, различные драйверы логирования, и способы выполнения команд в запущенном контейнере (`docker exec`, `docker attach`). Также обсудим особенности вывода stdout/stderr.

### Подключение к запущенному контейнеру (`exec` vs `attach`)

Если контейнер запущен в фоновом режиме (`-d`), часто требуется попасть внутрь него, например, чтобы проверить конфигурацию, выполнить отладочную команду, просмотреть файловую систему. Для этого используется:

- **`docker exec`** – выполнить новый процесс внутри контейнера. Синтаксис: `docker exec [опции] контейнер команда`. Чаще всего применяют `docker exec -it <container> /bin/sh` или `/bin/bash` для запуска интерактивного шелла внутри контейнера. Опции `-i` (interactive) и `-t` (allocate TTY) вместе позволяют «зайти» в контейнер как в терминал. Можно выполнять и разовые команды (без `-it`, просто `docker exec контейнер ls /data`).
  
  При `exec` контейнер **не перезапускается** и не останавливается – вы просто добавляете новый процесс. Основной процесс контейнера продолжает работать параллельно. Например, можно зайти в контейнер с веб-сервером, не прерывая обслуживание. Когда вы выходите из `bash` (exec-процесса), контейнер продолжает работать.

- **`docker attach`** – присоединиться к *консоле* главного процесса контейнера. Это означает, что вы начнёте видеть вывод (stdout/stderr) основного процесса и сможете посылать ему ввод (stdin), если он его читает. Проще говоря, `attach` «телепортирует» вас к тому же месту, где был бы контейнер, если бы он запущен без `-d`. Например, если запустить `docker run -it ubuntu bash`, а потом из другого терминала сделать `docker attach` к этому контейнеру – вы присоединитесь к уже запущенному `bash` и увидите тот же терминал.

  `attach` удобен, если вы забыли добавить `-it` при запуске интерактивного контейнера – можно прикрепиться после. Но в целом его используют редко. Стоит помнить, что прервать attach без остановки контейнера можно комбинацией **Ctrl+P Ctrl+Q** (это разомкнёт сессию). Иначе, если просто нажать Ctrl+C, вы пошлёте сигнал SIGINT в процесс контейнера.

Для отладки и администрирования обычно хватает `docker exec`. Он позволяет, к примеру, установить внутрь контейнера дополнительные утилиты (через `apt-get`), проверить сеть (`ping` до другого сервиса) и т.д. При выходе exec-процесс завершается, а контейнер остаётся как был.

В продуктиве, чтобы получить shell в контейнере, лучше использовать `docker exec` (он требует, чтобы контейнер уже работал; если упал – не зайти). Если контейнер упал, для отладки можно его запустить в режиме `docker run --rm -it --entrypoint sh образ`, чтобы открыть shell без запуска основного приложения.

### Просмотр логов (`docker logs`) и драйверы логирования

По умолчанию Docker собирает весь вывод контейнера (то, что приложение пишет в stdout/stderr). Этот вывод можно просмотреть командой **`docker logs <container>`**. Она покажет весь накопленный лог (с момента старта контейнера) или его часть с опциями:
- `-f` (follow) – «хвост» логов в режиме реального времени, как `tail -f`.
- `--tail N` – показать последние N строк.
- `--since 1h` – показать логи только за последний час, и т.д.

Важно понимать, что `docker logs` работает только потому, что Docker-демон сохраняет логи контейнера. По умолчанию используется **json-file** драйвер логирования ([Docker Logging: 101 Guide to Logs, Best Practices & More](https://sematext.com/guides/docker-logs/#:~:text=By%20default%2C%20Docker%20uses%20a,called%20%E2%80%9CWhat%E2%80%99s%20a%20Logging%20Driver%3F%E2%80%9D)). Он пишет каждую строку вывода контейнера в файл JSON (в упомянутом каталоге `/var/lib/docker/containers/<id>/<id>-json.log`). В записях логов помимо текста хранится метка времени и поток (stdout/stderr) ([Docker Logging: 101 Guide to Logs, Best Practices & More](https://sematext.com/guides/docker-logs/#:~:text=The%20example%20below%20shows%20JSON,file%20driver)). Docker `logs` читает этот файл и выводит вам.

Docker поддерживает разные **лог-драйверы**:
- `json-file` – дефолт, локальное хранение (можно настроить `max-size` и ротацию).
- `journald` – отправляет логи в системный журнал systemd (можно потом `journalctl -u docker CONTAINER_ID` например).
- `syslog` – пишет в syslog хоста.
- `awslogs`, `gcp`, `splunk`, `fluentd` и многие другие – напрямую отправляют логи в удалённые системы мониторинга/агрегации.
- `none` – отключает сбор логов (контейнер пишет в stdout/stderr, но Docker их не хранит).

Если используется драйвер, отправляющий логи вовне, команда `docker logs` может **не показывать** ничего, т.к. демон не хранит локальную копию (кроме случаев, когда включен dual logging, но по умолчанию он off) ([Logs and metrics | Docker Docs
](https://docs.docker.com/engine/logging/#:~:text=In%20some%20cases%2C%20,unless%20you%20take%20additional%20steps)). Например, если вы запустили контейнер с `--log-driver=syslog`, то `docker logs` скорее всего ничего не вернёт, а логи надо смотреть в syslog.

Для большинства случаев json-file удобен, но требует управления размером логов. Можно установить глобальные опции (в daemon.json) для ограничения размера логов, например:
```json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "10m",
    "max-file": "3"
  }
}
```
Это будет ротация – не более 3 файлов по 10 MB каждый на контейнер.

**Чтение stdout/stderr:** `docker logs` объединяет оба потока по времени. Можно заметить, что ошибки (stderr) не особо отделены, разве что в JSON есть поле `"stream": "stderr"`. Если нужно именно разделить, можно настроить приложение писать разные вещи в stdout и stderr, а затем уже обработка на стороне системы логирования.

Внутри контейнера стандартный вывод и поток ошибок вашего приложения обычно идут либо в консоль (stdout/stderr) либо в файлы. **Лучшей практикой** в Docker является писать логи приложения в stdout/stderr (например, веб-сервер Nginx по умолчанию пишет access.log и error.log в файлы, но официальный образ делает симлинки этих файлов на `/dev/stdout` и `/dev/stderr` соответственно ([Logs and metrics | Docker Docs
](https://docs.docker.com/engine/logging/#:~:text=The%20official%20,See%20the%20Dockerfile)), чтобы Docker мог собирать логи). Если приложение пишет только в файл внутри контейнера, то `docker logs` его не увидит. Тогда надо либо менять стратегию логирования приложения, либо использовать `docker cp` чтобы забрать файл, либо использовать сторонний агент, читающий файлы внутри контейнера.

**Примеры использования `docker logs`:**

```bash
# Смотреть логи контейнера myapp
docker logs myapp

# Следить за логами в реальном времени
docker logs -f --tail 50 myapp   # покажет последние 50 строк и будет обновляться
```

Если контейнер перезапускается, то `docker logs` (без `-f`) покажет все логи с начала и между рестартами подряд, можно увидеть, где он падал. А с `-f` при рестарте нужно перенапускать команду (либо она отключится, либо нужно -f заново, в Docker старых версий follow прерывался на рестарте).

### Особенности stdout/stderr в контейнерах

В контейнере основной процесс обычно наследует стандартные потоки от Docker. Если запустить контейнер без `-t` (без псевдо-tty), то stdout/stderr приложения будут **не привязаны к терминалу**. Это может влиять на буферизацию вывода: многие приложения в отсутствии терминала буферизуют вывод по блокам, а не построчно. Например, Python-приложение может задерживать вывод в stdout, пока буфер не заполнится, если нет tty. Поэтому иногда в логах Docker видно, что строки приходят пачками. Решение – либо запускать с `-t` (но тогда `docker logs` тоже может получать форматирование, управляющие символы), либо в самой программе явно флешить вывод, либо использовать универcальную переменную окружения `PYTHONUNBUFFERED=1` (для Python). 

При использовании `docker attach` с `-t` у контейнера, stdout обычно становится line-buffered, что удобно для интерактива.

**Stdout vs Stderr:** Docker не объединяет их – они хранятся раздельно в JSON (с указанием поля), но просматриваются вместе. Если нужно собирать ошибки отдельно, можно настроить фильтр на уровне системы логов (например, в Splunk разделять по метке). 

Стоит знать, что Docker не делает никакой обработки вашего вывода – всё, что приложение записало, окажется в логе. Исключение: Docker может добавлять префиксы с идентификаторами при выводе `docker service logs` (в Swarm) или docker-compose logs (имя сервиса, номера реплик). Но `docker logs` выводит ровно строки приложения.

Внутри контейнера устройства `/dev/stdout` и `/dev/stderr` обычно указывают на соответствующие дескрипторы 1 и 2, которые Docker подключил к лог-драйверу. Поэтому вышеупомянутый трюк с симлинками лог-файлов на `/dev/stdout` действительно перенаправляет в Docker-лог.

**Логи ротация:** если не настроить, файл json с логами может расти бесконечно. В долгоживущих контейнерах с активным логированием обязательно ставьте лимиты, иначе рано или поздно место на диске закончится из-за логов.

**Где смотреть логи на диске:** мы упоминали – `/var/lib/docker/containers/<ID>/<ID>-json.log`. Иногда, если контейнер очень большой лог генерирует, удобнее чтение через инструменты (jq, grep) прямо этого файла, но чаще `docker logs` достаточно.

**Логгирование драйверами:** если интегрируетесь с ELK/EFK, Kubernetes и т.д., Docker можно настроить писать прямо в fluentd, gelf или т.п., чтобы не делать лишних прослоек. Но в простых случаях проще оставить json-file и использовать сторонние агенты (Filebeat, Vector и т.д.) которые читают эти логи.

## Практика и советы

Наконец, несколько практических команд и советов, которые полезны при работе с Docker на продвинутом уровне:

- **Проверка размера слоя контейнера:** чтобы увидеть, сколько данных накопил контейнер в своём writable-слое, используйте `docker ps -s` (или `docker container ls -s`). В выводе в столбце SIZE указано что-то вроде *"10MB (virtual 200MB)"*. Первое число – размер изменений контейнера, второе – суммарный размер контейнера вместе с базовым образом. Большой writable-layer может говорить о том, что контейнер хранит данные, которые лучше вынести в volume. Кроме того, командой `docker system df` можно увидеть общий объём, занимаемый контейнерами, образами, томами, и сколько из этого можно очистить. Если же нужен размер конкретного тома – его можно оценить обычным `du -sh` на папке в `/var/lib/docker/volumes/` или запустив временный контейнер: `docker run --rm -v myvol:/data alpine du -sh /data`.

- **Изучение структуры образа (`docker history`):** команда `docker history <image>` покажет список слоёв образа с указанием, какая команда Dockerfile их создала и размер каждого слоя. Это очень полезно для оптимизации: вы видите, что, например, шаг `RUN apt install ...` добавил 200 MB, а очистка кешей не выполнена – значит можно поправить Dockerfile. Пример вывода:
  ```bash
  docker history myimage:latest
  IMAGE          CREATED          CREATED BY                                      SIZE
  abcd1234efgh   2 days ago       /bin/sh -c (install packages)                    150MB
  1234abcdef56   2 days ago       /bin/sh -c (copy source files)                   20MB
  <missing>      2 days ago       /bin/sh -c (base image layer)                    50MB
  ```
  Здесь можно увидеть, какие слои "viable" (не иметь ID, <missing> – значит base layers).
  
  Также `docker image inspect myimage:latest -f '{{.RootFS.Layers}}'` выведет список хешей слоёв, а `'{{.Size}}'` – суммарный размер. Но history понятнее.
  
  Если нужно проследить наследование образов: `docker image inspect` покажет `Parent` (для dangling промежуточных). Но сейчас обычно parentless, т.к. history хранит цепочку.

- **Оптимизация размера образов:**
  - **Clean up в одном слое:** При установке пакетов в Dockerfile старайтесь чистить кэш менеджеров пакетов и временные файлы *в том же RUN*, чтобы не оставить их в образе. Например:  
    ```Dockerfile
    RUN apt-get update && apt-get install -y build-essential wget && \
        apt-get clean && rm -rf /var/lib/apt/lists/*
    ``` 
    Если разбить на два RUN (install, потом clean), то первый слой уже будет содержать кеши, и даже после удаления во втором они останутся в слое-истории.
  - **Мульти-стейдж билды:** Используйте несколько стадий в Dockerfile. Например, собрать приложение (Go, Java, Node) в одном stage, а в финальный образ скопировать только исполняемый файл или артефакты. Это позволяет не тащить компиляторы и dev-библиотеки в конечный образ. Итоговый образ будет значительно меньше. Пример:
    ```Dockerfile
    FROM golang:1.19 AS builder
    WORKDIR /app
    COPY . .
    RUN go build -o myapp
    
    FROM alpine:3.17
    COPY --from=builder /app/myapp /usr/bin/myapp
    CMD ["myapp"]
    ```
    Здесь финальный образ ~5MB, несмотря на использование 300MB image для сборки.
  - **Выбор базового образа:** Не всегда надо брать full дистрибутив. Если приложение написано на Go или Rust (статически слинковано), Alpine или даже `scratch` (пустой) – подходят. Но обратная сторона: Alpine может создавать сложности с глибо (glibc vs musl) или отсутствием пакетов. Поэтому оцените: например, для Python лучше взять slim-версию Debian, чем Alpine, из-за совместимости с большинством пакетов.
  - **.dockerignore:** Добавьте в `.dockerignore` файлы и папки, которые не нужны для сборки (локальные конфиги, исходники тестов, `node_modules` при многослойной сборке и т.п.). Это уменьшит контекст сборки, а значит и размер образа (не окажется случайно чего-то лишнего).
  - **Количество слоёв:** Раньше существовало ограничение в 127 слоёв. Сейчас это почти не проблема, но очень большой Dockerfile (сотни RUN) может слегка замедлять сборку и загрузку. Можно объединять команды, но не в ущерб читаемости. Главное – минимизировать *размер*, а не число слоёв. Один слой = один шаг, и если он большой, то хоть 1 слой, хоть 5 – общий размер останется.
  - **Squash (объединение слоёв):** Docker имеет экспериментальную возможность squashing – объединить все слои образа в один. Это может быть полезно, чтобы скрыть историю слоёв (например, если там были секреты, удалённые позже, но их можно теоретически восстановить из предыдущего слоя), либо чтобы чуть сэкономить место на метаданных. Однако squash лишает преимуществ слоистой структуры (кэширование, повторное использование базовых слоёв). Сейчас чаще вместо squash просто стараются делать “чистые” образы через multi-stage.
  - **Удаление ненужных компонентов:** если вы ставите, к примеру, `apt-get install -y something` и он притянул 100 зависимостей, а ваше приложение использует только часть – возможно, можно удалить некоторую часть после установки (но аккуратно, чтобы приложение продолжило работать). Либо использовать опции пакетного менеджера `--no-install-recommends` для apt, или аналогичные флаги у других, чтобы меньше лишнего устанавливать.
  - **Анализ готовых образов:** есть утилиты (например, `dive`), позволяющие интерактивно просмотреть содержимое каждого слоя, выявить "чем набито". Это помогает понять, где можно ужать.
  
  Общая цель – чтобы итоговый образ содержал только то, что необходимо для выполнения приложения. Всё временное – либо удалено, либо осталось в промежуточных слоях (которых в final нет при multi-stage).

- **Сжатие (compression):** Образы передаются как tar-архивы при push/pull, Docker автоматически их сжимает. Но имейте в виду, что указанные размеры – это уже разжатые на диске. Можно принудительно сжать (docker save + gzip) для переноски вручную, но внутри registry всё и так оптимизировано.

- **docker system prune регулярно:** в dev-среде не забывайте чистить dangling images (их бывает много после сборки). `docker builder prune` чистит кэш сборки, если он у вас в overlay2 лежит (BuildKit кэширует тоже).

- **Следите за версиями образов:** не держите много старых версий. Используйте теги latest, v1, v2 и убирайте то, что совсем устарело (чтобы `image prune -a` не снёс нужное, можно пометить их через container or keep them loaded).

</details>
