# Введение в Nginx

В современном веб-разработке умение настроить и использовать веб-сервер **Nginx** является важным навыком. Nginx известен своей высокой производительностью и гибкостью. В данной лекции мы рассмотрим ключевые понятия, связанные с Nginx, и узнаем, почему он настолько популярен. Сначала мы разберемся, что такое прокси-сервер и в чём разница между прямым и обратным прокси. Затем перейдём к истории появления и возможностям Nginx, поговорим о различных уровнях проксирования (сетевом и прикладном), рассмотрим, как Nginx обслуживает статические файлы и повышает безопасность. Отдельно остановимся на том, как Nginx выполняет балансировку нагрузки между серверами. Наконец, обсудим преимущества и недостатки Nginx. Лекция рассчитана на начинающих, поэтому все термины будут объясняться простым языком, с примерами конфигурации для наглядности.

## 1. Что такое прокси-сервер?

**Прокси-сервер** – это промежуточный сервер, который находится между клиентом и целевым сервером и передает запросы от одного к другому. Проще говоря, когда клиент (например, ваш браузер) хочет получить данные с некоторого веб-сайта, он может обратиться не напрямую к этому сайту, а сначала к прокси-серверу. Прокси примет запрос от клиента и сам запросит нужные данные у целевого сервера, а затем вернет полученный ответ клиенту. Клиент как бы общается не напрямую с целевым сервером, а через посредника в лице прокси-сервера.

**Типы прокси-серверов:** Существуют два основных типа прокси:
- **Прямой прокси-сервер (forward proxy)** – находится ближе к клиенту. Клиенты (например, компьютеры в локальной сети) отправляют свои запросы на внешний интернет сначала этому серверу. Прямой прокси может выполнять роль шлюза для выхода в интернет: он отправляет запросы от имени клиентов, а затем возвращает им ответ. Такой прокси часто используется для кэширования внешних ресурсов или обхода ограничений доступа. Также прямой прокси скрывает реальный IP-адрес клиента от конечного сервера, обеспечивая некоторую анонимность.
- **Обратный прокси-сервер (reverse proxy)** – находится ближе к серверу (на стороне сервера). Клиенты отправляют запросы, считая, что обращаются к целевому серверу, но фактически эти запросы перехватывает обратный прокси. Обратный прокси получает запросы от внешних клиентов и перенаправляет их на один или несколько внутренних серверов, где находятся реальные данные или приложения. Ответы от внутренних серверов возвращаются прокси, а затем – клиенту. Таким образом, внешне для клиента прокси выглядит как сам целевой сервер. Обратные прокси используются для распределения нагрузки между несколькими серверами, защиты внутренних серверов и кэширования ответов.

**Основные задачи прокси-серверов:** Независимо от типа (прямого или обратного), прокси-серверы могут выполнять ряд полезных функций:
- **Кэширование:** Прокси может сохранять (кэшировать) часто запрашиваемые данные. Например, если множество клиентов запрашивает один и тот же ресурс (страницу или изображение), прокси-сервер может вернуть сохраненную копию этого ресурса из своего кэша, не обращаясь каждый раз к внешнему серверу. Это ускоряет выдачу контента и снижает нагрузку на исходный сервер.
- **Анонимизация:** Прокси обеспечивает определенную степень анонимности. В случае прямого прокси скрывается информация о клиенте (целевой сервер видит IP прокси, а не клиента). В случае обратного прокси скрываются детали инфраструктуры сервера (клиент не знает, какие конкретно внутренние серверы обрабатывают его запросы).
- **Контроль доступа и фильтрация:** Прокси-сервер может фильтровать трафик и управлять доступом. Например, корпоративный прямой прокси может блокировать доступ к нежелательным сайтам для сотрудников или вести журнал посещений. Обратный прокси может использоваться для ограничения доступа к определенным внутренним ресурсам, разрешая доступ только авторизованным клиентам.
- **Балансировка нагрузки:** В случае обратного прокси одна из ключевых задач – распределять входящие запросы между несколькими внутренними серверами, чтобы никакой из них не был перегружен. Это обеспечивает более высокую производительность и отказоустойчивость. Прямые прокси обычно не используются для балансировки, но могут распределять выходящие запросы через разные узлы при необходимости.

## 2. Зачем нужен Nginx?

Теперь, разобравшись с понятием прокси, перейдем к Nginx. **Nginx** (произносится как “Engine-X”) – это популярный веб-сервер и программный продукт, который изначально создавался как высокопроизводительный HTTP-сервер с функцией обратного проксирования и балансировки нагрузки. Nginx был разработан Игорем Сысоевым в начале 2000-х годов для решения проблемы эффективного обслуживания большого количества одновременных подключений (известной как проблема C10k – *10 000 одновременных соединений*). Первая версия Nginx вышла в 2004 году, и с тех пор он стал одним из наиболее широко используемых веб-серверов в мире. Nginx отличается событийно-ориентированной (асинхронной) архитектурой, что позволяет ему обрабатывать множество запросов с минимальными затратами ресурсов.

**Основные функции Nginx:** Благодаря своей архитектуре и возможностям, Nginx может выполнять несколько ролей в веб-инфраструктуре:
- **HTTP-сервер (веб-сервер):** Nginx способен самостоятельно обслуживать веб-сайты, отдавая клиентам статические файлы (HTML-страницы, изображения, стили, скрипты и т.д.). Он может выступать в роли основного сервера для статического контента или простых сайтов.
- **Обратный прокси-сервер:** Одна из самых распространенных ролей Nginx – принимать входящие HTTP-запросы от клиентов и проксировать (перенаправлять) их на другие серверы. Например, Nginx может стоять перед группой приложений (написанных на Node.js, Python, PHP и др.) и распределять запросы между ними. При этом Nginx может кешировать ответы от приложений и выдавать их повторно без запроса к бэкенду, если те же данные запрашиваются часто.
- **Балансировщик нагрузки:** Nginx умеет распределять входящие соединения между несколькими серверами приложений (выполняя функцию балансировки нагрузки, как уже упоминалось). Это позволяет масштабировать приложение горизонтально – добавляя больше серверов, мы можем обслужить больше пользователей, а Nginx будет решать, как направить каждого нового пользователя на менее загруженный узел.
- **Сервер статического контента:** Хотя это частично дублирует роль HTTP-сервера, отдельно стоит подчеркнуть эффективность Nginx при раздаче статических файлов. Nginx специально оптимизирован для быстрой выдачи файлов из памяти или с диска. Его используют в качестве фронтенд-сервера для статических ресурсов, даже если динамическая часть сайта работает на другом ПО. Например, Nginx может обслуживать картинки и CSS, а запросы за динамическими страницами пересылать другому серверу.

**Примеры использования Nginx:**  
Nginx на практике применяется в самых разных сценариях:
- Для сайтов с высокой нагрузкой и большим числом пользователей Nginx выступает в роли входной точки (reverse proxy) и балансировщика. Например, социальные сети, новостные порталы, крупные интернет-магазины используют Nginx, чтобы справиться с миллионами запросов в сутки.
- Для статических веб-сайтов или сервисов Nginx часто выбирают благодаря его эффективности. Если у вас простой сайт (например, портфолио или документация), Nginx может служить основным и единственным веб-сервером, быстро отдающим контент пользователям.
- В архитектуре микросервисов Nginx может выполнять роль шлюза (API Gateway), принимая запросы к разным сервисам и проксируя их в нужный микросервис, а также осуществляя кэширование общих ответов и обеспечивая безопасность. (Для некоторых из этих возможностей могут потребоваться дополнительные модули или коммерческая версия.)

## 3. Уровни проксирования (L7 и L3)

При обсуждении прокси-серверов и балансировки нагрузки часто упоминаются «уровни» проксирования, например, прокси уровня 7 и уровня 3. Эти уровни относятся к **OSI-модели** сетевого взаимодействия, которая разделяет процесс коммуникации на 7 слоев. Рассмотрим два из них, связанных с проксированием:
- **Прокси на уровне приложений (Layer 7, L7):** Это прокси, работающие на 7-м уровне OSI – уровне приложений. Они «понимают» прикладной протокол, с которым работают. В контексте веба – это HTTP (протокол прикладного уровня). Прокси L7 (например, Nginx в режиме обратного прокси для HTTP) может анализировать содержимое HTTP-запросов: URL, заголовки, данные. Благодаря этому решения на уровне приложений позволяют делать умное проксирование – например, перенаправлять запросы к разным серверам в зависимости от запрошенного пути (одни URL обрабатываются одним сервером, другие – другим), выполнять кэширование веб-страниц, сжатие ответов, добавлять или удалять заголовки. Проще говоря, L7-прокси – это программный посредник, полностью понимающий специфический протокол (HTTP, FTP, SMTP и т.д.) и способный анализировать и изменять передаваемые данные на прикладном уровне.
- **Прокси на сетевом уровне (Layer 3, L3):** Прокси или переадресация трафика на 3-м уровне – это более низкоуровневый подход. Уровень 3 – сетевой уровень OSI – оперирует пакетами данных, IP-адресами и маршрутизацией. Примером L3-прокси можно считать обычный сетевой шлюз (маршрутизатор) с настроенным NAT (Network Address Translation). Такой шлюз принимает пакеты от клиента и переправляет их к серверу, подставляя свои собственные сетевые реквизиты вместо клиентских, а ответы – обратно клиенту. Важно отметить, что на уровне 3 прокси *не разбирается в содержимом* пакетов: он не знает, какой конкретно запрос внутри или к какому URL он направлен – он лишь переводит адреса и порты. L3-прокси не может выборочно кэшировать веб-страницы или балансировать запросы по содержимому – он работает “слепо” с точки зрения приложений, просто переадресуя трафик. Преимущество такого подхода – высокая скорость и универсальность (можно передавать любой вид трафика), недостаток – отсутствие «умного» управления на уровне приложения. Nginx, будучи HTTP-сервером, по умолчанию работает на уровне 7 (HTTP), однако с помощью модуля `stream` он способен проксировать и сырой TCP/UDP-трафик (что ближе к транспортному/сетевому уровню, L4/L3, например, для целей проксирования базы данных или TLS-подключений без разбора HTTP).

Таким образом, *проксирование L7* подразумевает понимание и обработку трафика на уровне протоколов приложений, а *проксирование L3* – это более простое пересылание пакетов, не вникая в их содержимое. Веб-разработчикам важно знать, что Nginx – это в первую очередь L7-прокси (работающий с HTTP и подобными протоколами), который может принимать решения на основании данных запроса.

## 4. Проксирование статических файлов

В веб-приложениях часто статический контент (изображения, стили CSS, JavaScript-файлы, видео и др.) отделяется от динамического. **Проксирование статических файлов** означает, что веб-сервер (например, Nginx) берет на себя отдачу этих файлов пользователям, даже если основное приложение генерирует только динамические страницы. Зачем это нужно?

Во-первых, раздача статики через специализированный веб-сервер существенно эффективнее. Nginx оптимизирован для обслуживания большого количества мелких и больших файлов и может использовать кэширование и сжатие. Если же статикой будет заниматься само приложение (написанное, например, на Python или PHP), то ценное время CPU и памяти будет расходоваться на передачу файлов вместо того, чтобы генерировать новую динамическую логику. Поэтому обычно делают так: динамические запросы (например, к API или страницам) проксируются Nginx-ом на приложение, а запросы к файлам (например, к `https://example.com/static/style.css`) обслуживаются самим Nginx напрямую с диска.

Во-вторых, Nginx позволяет легко настроить кэширование статического контента на стороне клиента. В конфигурации можно указать заголовки, позволяющие браузеру сохранять полученные файлы определенное время, чтобы при повторном обращении к сайту не тратить трафик и время на загрузку одного и того же. Это повышает скорость загрузки страниц для пользователя и снижает нагрузку на сервер при повторных визитах.

**Настройка Nginx для обслуживания статики:** Рассмотрим простой пример конфигурации. Предположим, что статические файлы нашего сайта лежат в директории `/var/www/site/static` (там находятся изображения, CSS, JS и другие файлы), а динамические запросы должны идти на наше приложение, работающее на локальном порту 5000.

Мы можем написать в конфигурационном файле Nginx следующее:
```nginx
server {
    listen 80;
    server_name example.com;

    # Обслуживание статических файлов
    location /static/ {
        root /var/www/site;      # Директория сайта, содержащая папку static
        expires 1h;              # Разрешить браузеру кешировать эти файлы 1 час
        access_log off;          # (Опционально) отключаем логирование обращений к статике
    }

    # Проксирование остальных запросов на приложение
    location / {
        proxy_pass http://127.0.0.1:5000;  # Все остальные URL идут на приложение
    }
}
```
В этом примере:  
- Блок `location /static/` означает, что для всех запросов, начинающихся с пути `/static/`, Nginx будет искать соответствующие файлы в директории `/var/www/site/static` (поскольку указан `root /var/www/site`, к которому добавляется запрошенный путь). Например, запрос к `http://example.com/static/img/logo.png` приведет к выдаче файла `/var/www/site/static/img/logo.png` напрямую с сервера (если такой файл существует).
- Директива `expires 1h;` указывает, что во все ответы на такие запросы Nginx добавит заголовок, позволяющий кэшировать полученный файл в браузере на протяжении 1 часа. Это улучшает производительность на стороне клиента и снижает повторную нагрузку на сервер.
- Остальные запросы (не начинающиеся на `/static/`) попадут под правило `location /` и будут проксированы на локальное приложение (работающее на порту 5000). Таким образом, динамическая часть обрабатывается приложением, а статическая – самим Nginx.

Такое разделение позволяет эффективно обслуживать пользователей: статические ресурсы отдаются быстро и с кэшированием, а приложению остается больше ресурсов для обработки бизнес-логики. В реальных конфигурациях могут применяться и другие директивы – например, `try_files` для поиска файла или отдачи заглушки, или `alias` вместо `root` – но принцип остается тем же.

## 5. Безопасность с Nginx

Nginx также играет важную роль в обеспечении безопасности веб-приложения. На уровне веб-сервера можно настроить ряд механизмов, повышающих защиту и устойчивость к атакам.

**Ограничение частоты запросов:** Один из методов противодействия DoS-атакам или недобросовестному использованию API – ограничение количества запросов от одного клиента. Nginx предоставляет модуль `ngx_http_limit_req_module`, позволяющий *дросселировать* (ограничивать) слишком частые запросы. С помощью директивы `limit_req_zone` создается зона (область в памяти) для хранения состояния запросов. Например, мы можем объявить в контексте `http`:
```nginx
limit_req_zone $binary_remote_addr zone=mylimit:10m rate=5r/s;
```
Эта строка создаст зону `mylimit` размером 10 МБ, где ключом будет IP-адрес клиента (`$binary_remote_addr` – адрес клиента в бинарном виде), и установит средний лимит **5 запросов в секунду** для каждого уникального IP. Далее, для конкретных локаций можно применить это ограничение:
```nginx
server {
    location /api/ {
        limit_req zone=mylimit burst=10 nodelay;
        proxy_pass http://127.0.0.1:5001;
    }
}
```
В данном примере для всех запросов к `/api/` будет действовать ограничение: не более 5 запросов в секунду с одного IP. Параметр `burst=10` допускает временное превышение лимита на 10 запросов (они будут помещены в очередь), а `nodelay` указывает не задерживать выполнение запросов сверх лимита, а сразу отклонять лишние (с кодом 503). Таким образом, чрезмерно частые запросы от одного клиента могут быть автоматически замедлены или отвергнуты, что защищает ваш бэкенд от перегрузки.

**Сокрытие инфраструктуры:** Как мы отмечали, обратный прокси скрывает внутренние узлы (серверы) от клиента. Это само по себе плюс безопасности: пользователи и потенциальные злоумышленники не знают прямых адресов ваших приложений – они видят только один внешний узел (Nginx). Кроме того, Nginx позволяет скрывать информацию о себе. По умолчанию Nginx может отправлять в заголовках или на страницах ошибок свой идентификатор и версию (например, `"Server: nginx/1.20.1"`). С помощью директивы `server_tokens off;` можно отключить отображение версии, чтобы не облегчать злоумышленнику задачу идентификации вашего веб-сервера. Также можно на уровне конфигурации фильтровать или убирать некоторые заголовки, исходящие от внутренних серверов, если они выдают лишнюю информацию (например, о версии PHP или другого ПО).

**Использование HTTPS/SSL:** Безопасность передачи данных – критически важный аспект. Nginx полностью поддерживает шифрование трафика по протоколу HTTPS (HTTP поверх TLS/SSL). Настроить HTTPS в Nginx относительно просто, получив действующий сертификат для вашего домена (сегодня это можно сделать бесплатно, например, с помощью сервиса Let's Encrypt). В конфигурации сервера для включения SSL нужны директивы примерно следующего вида:
```nginx
server {
    listen 443 ssl;
    server_name example.com;
    ssl_certificate /etc/ssl/certs/example.com.crt;
    ssl_certificate_key /etc/ssl/private/example.com.key;

    location / {
        # ... основная настройка: proxy_pass или root для контента
    }
}
```
Здесь мы указываем, что сервер слушает 443-й порт с SSL (`listen 443 ssl;`), задаем имя сервера (домен), и подключаем файлы сертификата и ключа. После такой настройки Nginx будет принимать безопасные HTTPS-соединения. Конечно, в реальности рекомендуется дополнительно настроить список шифров, протоколы (например, отключить устаревшие TLS 1.0/1.1) и переадресацию с нешифрованного порта 80 на 443, но базовый принцип именно такой.

Благодаря SSL, данные между клиентом и сервером шифруются, что предотвращает их перехват или подмену. Nginx выступает завершающим узлом SSL-сессии: он расшифровывает запрос клиента, а затем может либо сам обработать его (если это статический файл), либо передать его дальше во внутреннее приложение. В ответе, перед отправкой клиенту, Nginx вновь шифрует данные. Таким образом, внутренние компоненты могут не беспокоиться о шифровании – эту задачу решает обратный прокси.

## 6. Nginx как балансировщик нагрузки

Когда количество пользователей или объем запросов к веб-приложению растет, одиночный сервер может перестать справляться. Для масштабирования приложения по горизонтали (т.е. добавления новых серверов) необходим **балансировщик нагрузки** – компонент, который будет распределять входящие запросы между несколькими узлами. Nginx отлично подходит на эту роль, работая как программный балансировщик на уровне HTTP (L7).

Основные алгоритмы балансировки нагрузки, поддерживаемые в Nginx:
- **Round Robin (круговой алгоритм):** По умолчанию Nginx распределяет запросы по принципу round robin – то есть по очереди перебирает список доступных серверов. Первый запрос – на первый сервер, второй – на второй, и так далее, а затем цикл повторяется с начала списка. Такой равномерный цикл обеспечивает примерно одинаковую нагрузку, если запросы от разных пользователей схожи по тяжести обработки.
- **Least Connections (наименьшее число соединений):** При использовании этого алгоритма Nginx будет направлять новый запрос тому серверу, у которого в текущий момент наименьшее количество активных (незавершенных) соединений с клиентами. Предполагается, что такой сервер менее загружен. Данный подход хорош, если время обработки запросов заметно варьируется: например, один сервер может обслуживать несколько "тяжелых" запросов и еще не завершил их, тогда как другой уже свободен – новый клиент будет направлен на более свободный сервер, чтобы выровнять нагрузку.
- **IP Hash (по хешу IP-адреса):** Этот метод привязывает клиентов к определенным серверам на основе IP-адреса. Алгоритм берет IP-адрес клиента, вычисляет из него хеш и на основании этого определяет, на какой сервер отправить запрос. Таким образом, один и тот же клиент (с одним и тем же IP) будет всегда попадать на один и тот же внутренний сервер. Этот подход полезен для сохранения состояния (session stickiness), когда важно, чтобы последовательные запросы пользователя обрабатывались одним сервером (например, если сессии хранятся в памяти того же сервера). Минус – при смене IP клиента или неравномерном распределении пользователей по IP нагрузка может делиться неидеально.

Настройка балансировки в Nginx осуществляется с помощью блока **`upstream`** и директивы `proxy_pass`. Сначала в конфигурации объявляется группа серверов (upstream), например:
```nginx
http {
    upstream backend_cluster {
        least_conn;
        server 192.168.1.101;
        server 192.168.1.102;
    }

    server {
        listen 80;
        server_name myservice.com;
        location / {
            proxy_pass http://backend_cluster;
        }
    }
}
```
В примере выше мы создали upstream-группу `backend_cluster`, куда включили два сервера (с IP-адресами 192.168.1.101 и 192.168.1.102). Директива `least_conn;` внутри блока `upstream` задает алгоритм наименьшего числа соединений для распределения нагрузки. Если ее убрать, будет применяться round robin по умолчанию. Далее, в секции `server`, мы указываем, что все запросы (`location /`) проксируются (`proxy_pass`) на адрес `http://backend_cluster`, то есть на нашу группу серверов. Nginx будет сам выбирать конкретный адрес из группы согласно заданному алгоритму.

Можно также явно использовать `ip_hash;` в блоке upstream, если нужна привязка по IP, или назначить **вес** для каждого сервера (например, `server 192.168.1.101 weight=2;` чтобы на сервер 192.168.1.101 шло вдвое больше запросов, чем на остальные). Все эти механизмы позволяют тонко настроить распределение нагрузки под потребности вашего приложения.

По сути, с Nginx вы можете добиться программной балансировки без необходимости приобретать отдельное оборудование. Его производительности достаточно для большинства случаев, а настройка – гибкая. Важно учитывать мониторинг: Nginx может отмечать серверы как **неработоспособные** (down), если они перестают отвечать, и временно исключать их из баланса. В версии с открытым исходным кодом проверка здоровья серверов осуществляется на основе неудачных попыток подключения, тогда как Nginx Plus предоставляет более широкие возможности проверок состояния.

## 7. Достоинства и недостатки Nginx

Как и у любого инструмента, у Nginx есть сильные и слабые стороны. Ключевые из них:

**Преимущества Nginx:**
- **Высокая производительность:** Благодаря асинхронной архитектуре Nginx потребляет мало памяти и эффективно использует ресурсы при обработке множества соединений. Он способен обслуживать очень большое число одновременных запросов, не теряя в скорости. На практике Nginx часто превосходит традиционные серверы (например, Apache) по скорости отдачи статического контента и стабильности под высокой нагрузкой.
- **Гибкость конфигурации:** Nginx предоставляет богатый набор модулей и директив, позволяющих настроить практически любой аспект работы сервера. Можно определять разные правила для разных частей сайта, перенаправлять запросы, переписывать URL, управлять заголовками, устанавливать кэширование, ограничения на запросы и многое другое. При этом один экземпляр Nginx способен обслуживать множество сайтов (виртуальных хостов), разграничивая их по доменным именам.
- **Масштабируемость и экономичность:** Nginx хорошо масштабируется на многопроцессорных системах и может работать в кластере. Добавляя больше рабочих процессов или серверов, вы линейно увеличиваете обрабатываемую нагрузку. При этом Nginx экономно расходует оперативную память и CPU, что позволяет ему работать даже на недорогом оборудовании или виртуальных машинах с ограниченными ресурсами.
- **Многофункциональность:** Одно и то же программное обеспечение может выступать сразу в нескольких ролях (веб-сервер, прокси, балансировщик, почтовый прокси). Кроме HTTP, Nginx умеет проксировать почтовые протоколы (IMAP, SMTP) и сырой TCP/UDP трафик. Это снижает сложность инфраструктуры: во многих случаях достаточно настроить Nginx, без необходимости в отдельных решениях для каждой задачи.

**Недостатки Nginx:**
- **Ограниченная расширяемость модулей:** Исторически в Nginx отсутствовала возможность легкого подключения внешних модулей без пересборки программы. Современные версии поддерживают *динамические модули*, но их все равно нужно компилировать отдельно под нужную версию Nginx. Это значит, что расширить функциональность (например, добавить нестандартный модуль) сложнее, чем в некоторых других серверах. Кроме того, Nginx не выполняет на своей стороне код веб-приложений (нет встроенного PHP или аналогов) – для обработки динамического контента всегда требуется отдельный backend (например, PHP-FPM для PHP, или приложение на Python/Node.js). Такой подход более безопасен и производителен, но может усложнить настройку для новичков.
- **Кривая обучения для новичков:** Синтаксис конфигурации Nginx довольно лаконичен, но включает свои нюансы (приоритеты разных location, директивы в разных контекстах, тонкости переписи URL и пр.). Новичкам бывает непросто сразу понять, почему правило не работает, или как правильно протестировать конфигурацию. Также Nginx не имеет такой распространенной мелкомасштабной настройки, как `.htaccess` в Apache, поэтому любую правку приходится вносить в основной конфигурационный файл и перезагружать сервер. Это требует аккуратности и понимания общей структуры конфигов.
- **Часть возможностей – в платной версии:** Некоторые продвинутые функции (как то: удобный мониторинг через дашборд, расширенные проверки состояния backend-серверов, поддержка динамического перенастроения без перезагрузки, встроенный WAF и пр.) доступны только в коммерческом продукте **NGINX Plus**. Открытая версия полностью бесплатна и покрывает почти все типичные потребности веб-сервера, но если ваша инфраструктура требует именно тех возможностей, которые есть только в Plus, может понадобиться покупка лицензии или использование дополнительных внешних инструментов.

Подводя итог, Nginx предлагает выдающуюся производительность и гибкость, благодаря чему стал выбором по умолчанию для многих проектов. Однако его использование предполагает понимание принципов работы и конфигурации. Зная сильные и слабые стороны Nginx, вы сможете эффективнее применять его в своих проектах и избегать потенциальных подводных камней.

## 8. Заключение и вопросы

Мы рассмотрели основные темы, связанные с введением в Nginx:
- Дали определение прокси-сервера, разобрали отличия между прямым и обратным прокси и узнали, какие задачи решает проксирование (кэширование, анонимность, фильтрация, балансировка нагрузки).
- Познакомились с историей появления Nginx и его назначением. Узнали, что Nginx – это высокопроизводительный веб-сервер и обратный прокси, способный работать со множеством одновременных соединений благодаря своей архитектуре.
- Выяснили, чем отличаются прокси на уровне приложений (L7) от прокси на сетевом уровне (L3), и что Nginx в основном функционирует на прикладном уровне, «понимая» HTTP-трафик.
- Узнали, как Nginx может обслуживать статические файлы, и почему вынос статики на отдельный сервер (или на фронтэнд-сервер) улучшает производительность веб-приложения.
- Обсудили аспекты безопасности при использовании Nginx: ограничение частоты запросов (для защиты от перегрузки), скрытие информации о сервере и настройку HTTPS для шифрования данных.
- Рассмотрели роль Nginx как балансировщика нагрузки, ознакомились с разными алгоритмами распределения запросов (round robin, least connections, ip hash) и примером конфигурации для балансировки между несколькими серверами.
- Проанализировали достоинства Nginx (производительность, гибкость, масштабируемость, универсальность) и его недостатки (сложность расширения, порог входа для новичков, отсутствие некоторых функций в бесплатной версии).


```nginx
# Основной конфигурационный файл Nginx

# Пользователь, от имени которого работает nginx
user www-data;

# Количество рабочих процессов (auto = столько, сколько ядер CPU)
worker_processes auto;

# Логирование процессов
error_log /var/log/nginx/error.log warn;
pid /run/nginx.pid;

# Событийная модель подключения (обычно оставляют как есть)
events {
    worker_connections 1024;  # Максимум соединений на один рабочий процесс
}

http {
    # Типы MIME для контента
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Логи доступа (можно включить/выключить)
    access_log /var/log/nginx/access.log;

    # Сжатие ответов gzip
    gzip on;

    # -------------------------
    # Безопасность: ограничение количества запросов
    # -------------------------
    limit_req_zone $binary_remote_addr zone=mylimit:10m rate=5r/s;
    # Храним IP клиентов ($binary_remote_addr), выделяем 10 МБ под эту зону,
    # разрешаем 5 запросов в секунду на IP

    # -------------------------
    # Балансировка нагрузки: upstream блок
    # -------------------------
    upstream backend_app {
        least_conn;  # Используем алгоритм наименьшей загрузки
        server 192.168.1.101;
        server 192.168.1.102;
    }

    # -------------------------
    # Виртуальный хост — HTTP
    # -------------------------
    server {
        listen 80;
        server_name example.com www.example.com;

        # Перенаправление HTTP -> HTTPS
        return 301 https://$host$request_uri;
    }

    # -------------------------
    # Виртуальный хост — HTTPS
    # -------------------------
    server {
        listen 443 ssl;
        server_name example.com www.example.com;

        # SSL-сертификаты
        ssl_certificate     /etc/ssl/certs/example.com.crt;
        ssl_certificate_key /etc/ssl/private/example.com.key;

        # Безопасность: отключение отображения версии сервера
        server_tokens off;

        # -------------------------
        # Обработка статики
        # -------------------------
        location /static/ {
            root /var/www/mysite;   # Файлы по пути /var/www/mysite/static/
            expires 30d;            # Кэширование в браузере на 30 дней
            add_header Cache-Control "public";
            access_log off;         # Отключаем логирование статики (опционально)
        }

        # -------------------------
        # Ограничение на API-запросы
        # -------------------------
        location /api/ {
            limit_req zone=mylimit burst=10 nodelay;  # Не более 5r/s + 10 в очереди
            proxy_pass http://backend_app;            # Проксирование в upstream
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # -------------------------
        # Проксирование всех остальных запросов
        # -------------------------
        location / {
            proxy_pass http://backend_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }

        # -------------------------
        # Обработка ошибок
        # -------------------------
        error_page 404 /custom_404.html;
        location = /custom_404.html {
            root /var/www/mysite;
        }
    }
}
```
